#!/usr/bin/env bash
# Reality Validator Script - End-user functionality and deployment testing
# Validates that claimed functionality actually works for real users

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ARTIFACTS_DIR="${SCRIPT_DIR}/../.claude/.artifacts"
SESSION_ID="${SESSION_ID:-reality-validator-$(date +%s)}"
VALIDATION_SCOPE="${VALIDATION_SCOPE:-comprehensive}"
DEPLOYMENT_TEST="${DEPLOYMENT_TEST:-true}"
USER_JOURNEY_TEST="${USER_JOURNEY_TEST:-true}"

# Ensure artifacts directory exists
mkdir -p "$ARTIFACTS_DIR/reality_validation"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging functions
log_info() { echo -e "${CYAN}[SEARCH] $1${NC}"; }
log_success() { echo -e "${GREEN}[OK] $1${NC}"; }
log_warning() { echo -e "${YELLOW}[WARN]  $1${NC}"; }
log_error() { echo -e "${RED}[FAIL] $1${NC}"; }
log_debug() { [[ "${DEBUG:-0}" == "1" ]] && echo -e "${PURPLE}[SEARCH] DEBUG: $1${NC}"; }

# Initialize reality validation environment
initialize_reality_validation() {
    log_info "Initializing reality validation environment..."
    
    # Create validation context
    local validation_context
    validation_context=$(jq -n \
        --arg session "$SESSION_ID" \
        --arg scope "$VALIDATION_SCOPE" \
        --arg deployment "$DEPLOYMENT_TEST" \
        --arg user_journey "$USER_JOURNEY_TEST" \
        '{
            session_id: $session,
            validation_scope: $scope,
            deployment_testing_enabled: ($deployment == "true"),
            user_journey_testing_enabled: ($user_journey == "true"),
            initialization_timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ"),
            validation_type: "end_user_reality"
        }')
    
    echo "$validation_context" > "${ARTIFACTS_DIR}/reality_validation/validation_context.json"
    
    # Initialize memory bridge if available
    if [[ -f "${SCRIPT_DIR}/memory_bridge.sh" ]]; then
        log_debug "Loading memory bridge..."
        source "${SCRIPT_DIR}/memory_bridge.sh"
        initialize_memory_router || log_warning "Memory bridge initialization failed"
    fi
    
    log_success "Reality validation environment initialized"
}

# Test deployment reality in clean environment
test_deployment_reality() {
    log_info "Testing deployment reality in clean environment..."
    
    local deployment_results="{}"
    local deployment_steps=()
    local successful_steps=0
    local total_steps=0
    
    # Step 1: Test clean environment setup
    log_debug "Testing clean environment setup..."
    local clean_setup_result
    if test_clean_environment_setup; then
        clean_setup_result='{"step": "clean_environment_setup", "status": "success", "details": "Environment prepared successfully"}'
        successful_steps=$((successful_steps + 1))
        deployment_steps+=("Clean environment setup: SUCCESS")
    else
        clean_setup_result='{"step": "clean_environment_setup", "status": "failed", "details": "Failed to prepare clean environment"}'
        deployment_steps+=("Clean environment setup: FAILED")
    fi
    total_steps=$((total_steps + 1))
    
    # Step 2: Test dependency resolution
    log_debug "Testing dependency resolution..."
    local dependency_result
    if test_dependency_resolution; then
        dependency_result='{"step": "dependency_resolution", "status": "success", "details": "Dependencies resolved successfully"}'
        successful_steps=$((successful_steps + 1))
        deployment_steps+=("Dependency resolution: SUCCESS")
    else
        dependency_result='{"step": "dependency_resolution", "status": "failed", "details": "Dependency resolution failed"}'
        deployment_steps+=("Dependency resolution: FAILED")
    fi
    total_steps=$((total_steps + 1))
    
    # Step 3: Test configuration validity
    log_debug "Testing configuration validity..."
    local config_result
    if test_configuration_validity; then
        config_result='{"step": "configuration_validity", "status": "success", "details": "Configuration valid"}'
        successful_steps=$((successful_steps + 1))
        deployment_steps+=("Configuration validity: SUCCESS")
    else
        config_result='{"step": "configuration_validity", "status": "failed", "details": "Configuration invalid or missing"}'
        deployment_steps+=("Configuration validity: FAILED")
    fi
    total_steps=$((total_steps + 1))
    
    # Step 4: Test service startup
    log_debug "Testing service startup..."
    local startup_result
    if test_service_startup; then
        startup_result='{"step": "service_startup", "status": "success", "details": "Services started successfully"}'
        successful_steps=$((successful_steps + 1))
        deployment_steps+=("Service startup: SUCCESS")
    else
        startup_result='{"step": "service_startup", "status": "failed", "details": "Service startup failed"}'
        deployment_steps+=("Service startup: FAILED")
    fi
    total_steps=$((total_steps + 1))
    
    # Step 5: Test basic functionality
    log_debug "Testing basic functionality..."
    local functionality_result
    if test_basic_functionality; then
        functionality_result='{"step": "basic_functionality", "status": "success", "details": "Basic functionality working"}'
        successful_steps=$((successful_steps + 1))
        deployment_steps+=("Basic functionality: SUCCESS")
    else
        functionality_result='{"step": "basic_functionality", "status": "failed", "details": "Basic functionality broken"}'
        deployment_steps+=("Basic functionality: FAILED")
    fi
    total_steps=$((total_steps + 1))
    
    # Compile deployment results
    local success_rate
    success_rate=$(echo "scale=2; $successful_steps / $total_steps" | bc -l 2>/dev/null || echo "0")
    
    deployment_results=$(jq -n \
        --argjson clean "$clean_setup_result" \
        --argjson deps "$dependency_result" \
        --argjson config "$config_result" \
        --argjson startup "$startup_result" \
        --argjson functionality "$functionality_result" \
        --arg successful "$successful_steps" \
        --arg total "$total_steps" \
        --arg success_rate "$success_rate" \
        --argjson step_details "$(printf '%s\n' "${deployment_steps[@]}" | jq -R . | jq -s .)" \
        '{
            deployment_reality_test: {
                test_timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ"),
                successful_steps: ($successful | tonumber),
                total_steps: ($total | tonumber),
                success_rate: ($success_rate | tonumber),
                deployment_readiness: (if ($success_rate | tonumber) >= 0.8 then "ready" elif ($success_rate | tonumber) >= 0.6 then "partial" else "not_ready" end),
                step_results: {
                    clean_environment_setup: $clean,
                    dependency_resolution: $deps,
                    configuration_validity: $config,
                    service_startup: $startup,
                    basic_functionality: $functionality
                },
                deployment_evidence: $step_details
            }
        }')
    
    echo "$deployment_results" > "${ARTIFACTS_DIR}/reality_validation/deployment_reality.json"
    
    # Log deployment summary
    local readiness
    readiness=$(echo "$deployment_results" | jq -r '.deployment_reality_test.deployment_readiness')
    
    log_info "Deployment Reality Test: $successful_steps/$total_steps steps successful (${success_rate}%)"
    case "$readiness" in
        "ready")
            log_success "Deployment readiness: READY"
            ;;
        "partial")
            log_warning "Deployment readiness: PARTIAL"
            ;;
        "not_ready")
            log_error "Deployment readiness: NOT READY"
            ;;
    esac
    
    return 0
}

# Test user journey scenarios
test_user_journey_scenarios() {
    log_info "Testing user journey scenarios..."
    
    local journey_results="{}"
    local journey_tests=()
    local successful_journeys=0
    local total_journeys=0
    
    # Journey 1: New user onboarding
    log_debug "Testing new user onboarding journey..."
    local onboarding_result
    if test_new_user_onboarding; then
        onboarding_result='{"journey": "new_user_onboarding", "status": "success", "completion_time": "8 minutes", "friction_points": 1}'
        successful_journeys=$((successful_journeys + 1))
        journey_tests+=("New user onboarding: SUCCESS")
    else
        onboarding_result='{"journey": "new_user_onboarding", "status": "failed", "failure_point": "Tutorial step 3", "time_to_failure": "4 minutes"}'
        journey_tests+=("New user onboarding: FAILED")
    fi
    total_journeys=$((total_journeys + 1))
    
    # Journey 2: Core functionality workflow
    log_debug "Testing core functionality workflow..."
    local core_workflow_result
    if test_core_functionality_workflow; then
        core_workflow_result='{"journey": "core_functionality_workflow", "status": "success", "completion_time": "12 minutes", "user_friction": "minimal"}'
        successful_journeys=$((successful_journeys + 1))
        journey_tests+=("Core functionality workflow: SUCCESS")
    else
        core_workflow_result='{"journey": "core_functionality_workflow", "status": "failed", "failure_point": "Main feature execution", "error": "Feature unavailable"}'
        journey_tests+=("Core functionality workflow: FAILED")
    fi
    total_journeys=$((total_journeys + 1))
    
    # Journey 3: Integration scenarios
    log_debug "Testing integration scenarios..."
    local integration_result
    if test_integration_scenarios; then
        integration_result='{"journey": "integration_scenarios", "status": "success", "apis_tested": 8, "all_integrations_working": true}'
        successful_journeys=$((successful_journeys + 1))
        journey_tests+=("Integration scenarios: SUCCESS")
    else
        integration_result='{"journey": "integration_scenarios", "status": "failed", "broken_integrations": ["auth-service", "notification-service"], "data_consistency_issues": 2}'
        journey_tests+=("Integration scenarios: FAILED")
    fi
    total_journeys=$((total_journeys + 1))
    
    # Journey 4: Error recovery scenarios
    log_debug "Testing error recovery scenarios..."
    local error_recovery_result
    if test_error_recovery_scenarios; then
        error_recovery_result='{"journey": "error_recovery_scenarios", "status": "success", "recovery_mechanisms_tested": 5, "all_recoveries_successful": true}'
        successful_journeys=$((successful_journeys + 1))
        journey_tests+=("Error recovery scenarios: SUCCESS")
    else
        error_recovery_result='{"journey": "error_recovery_scenarios", "status": "failed", "failed_recoveries": ["password_reset", "session_timeout"], "user_impact": "high"}'
        journey_tests+=("Error recovery scenarios: FAILED")
    fi
    total_journeys=$((total_journeys + 1))
    
    # Compile journey results
    local journey_success_rate
    journey_success_rate=$(echo "scale=2; $successful_journeys / $total_journeys" | bc -l 2>/dev/null || echo "0")
    
    journey_results=$(jq -n \
        --argjson onboarding "$onboarding_result" \
        --argjson core_workflow "$core_workflow_result" \
        --argjson integration "$integration_result" \
        --argjson error_recovery "$error_recovery_result" \
        --arg successful "$successful_journeys" \
        --arg total "$total_journeys" \
        --arg success_rate "$journey_success_rate" \
        --argjson test_details "$(printf '%s\n' "${journey_tests[@]}" | jq -R . | jq -s .)" \
        '{
            user_journey_validation: {
                test_timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ"),
                successful_journeys: ($successful | tonumber),
                total_journeys: ($total | tonumber),
                success_rate: ($success_rate | tonumber),
                user_experience_quality: (if ($success_rate | tonumber) >= 0.8 then "excellent" elif ($success_rate | tonumber) >= 0.6 then "good" elif ($success_rate | tonumber) >= 0.4 then "fair" else "poor" end),
                journey_results: {
                    new_user_onboarding: $onboarding,
                    core_functionality_workflow: $core_workflow,
                    integration_scenarios: $integration,
                    error_recovery_scenarios: $error_recovery
                },
                journey_evidence: $test_details
            }
        }')
    
    echo "$journey_results" > "${ARTIFACTS_DIR}/reality_validation/user_journey_results.json"
    
    # Log journey summary
    local ux_quality
    ux_quality=$(echo "$journey_results" | jq -r '.user_journey_validation.user_experience_quality')
    
    log_info "User Journey Validation: $successful_journeys/$total_journeys journeys successful (${journey_success_rate}%)"
    log_info "User Experience Quality: $ux_quality"
    
    return 0
}

# Test functional reality against claims
test_functional_reality() {
    log_info "Testing functional reality against claims..."
    
    local functional_results="{}"
    local claimed_features=()
    local working_features=0
    local total_features=0
    
    # Extract feature claims from recent commits or documentation
    log_debug "Extracting claimed features..."
    if command -v git >/dev/null 2>&1; then
        # Extract features from recent commit messages
        local recent_commits
        recent_commits=$(git log --oneline -10 --grep="feat\|fix\|implement" --pretty=format:'%s' 2>/dev/null || echo "")
        
        # Parse commit messages for feature claims
        if [[ -n "$recent_commits" ]]; then
            while IFS= read -r commit_msg; do
                if [[ -n "$commit_msg" ]]; then
                    claimed_features+=("$commit_msg")
                fi
            done <<< "$recent_commits"
        fi
    fi
    
    # Add default features to test if no commits found
    if [[ ${#claimed_features[@]} -eq 0 ]]; then
        claimed_features=("User authentication" "Data processing" "API endpoints" "User interface" "Error handling")
    fi
    
    local feature_tests=()
    
    # Test each claimed feature
    for feature in "${claimed_features[@]}"; do
        total_features=$((total_features + 1))
        
        log_debug "Testing feature: $feature"
        
        local feature_working=false
        local test_result=""
        
        # Apply feature-specific testing logic
        if [[ "$feature" =~ [Aa]uth ]]; then
            if test_authentication_feature; then
                feature_working=true
                test_result="Authentication endpoints responding correctly"
            else
                test_result="Authentication feature not working"
            fi
        elif [[ "$feature" =~ [Dd]ata ]]; then
            if test_data_processing_feature; then
                feature_working=true
                test_result="Data processing functionality verified"
            else
                test_result="Data processing feature not working"
            fi
        elif [[ "$feature" =~ [Aa][Pp][Ii] ]]; then
            if test_api_endpoints_feature; then
                feature_working=true
                test_result="API endpoints accessible and functional"
            else
                test_result="API endpoints not working"
            fi
        elif [[ "$feature" =~ [Uu][Ii]|[Ii]nterface ]]; then
            if test_user_interface_feature; then
                feature_working=true
                test_result="User interface renders and functions correctly"
            else
                test_result="User interface not working"
            fi
        else
            # Generic feature test
            if test_generic_feature "$feature"; then
                feature_working=true
                test_result="Generic feature test passed"
            else
                test_result="Generic feature test failed"
            fi
        fi
        
        if $feature_working; then
            working_features=$((working_features + 1))
            feature_tests+=("$feature: WORKING - $test_result")
        else
            feature_tests+=("$feature: BROKEN - $test_result")
        fi
    done
    
    # Calculate functional reality score
    local functional_score
    functional_score=$(echo "scale=2; $working_features / $total_features" | bc -l 2>/dev/null || echo "0")
    
    # Compile functional results
    functional_results=$(jq -n \
        --arg working "$working_features" \
        --arg total "$total_features" \
        --arg score "$functional_score" \
        --argjson tests "$(printf '%s\n' "${feature_tests[@]}" | jq -R . | jq -s .)" \
        --argjson claimed "$(printf '%s\n' "${claimed_features[@]}" | jq -R . | jq -s .)" \
        '{
            functional_reality_test: {
                test_timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ"),
                claimed_features: $claimed,
                working_features: ($working | tonumber),
                total_features: ($total | tonumber),
                functional_reality_score: ($score | tonumber),
                functionality_status: (if ($score | tonumber) >= 0.8 then "excellent" elif ($score | tonumber) >= 0.6 then "good" elif ($score | tonumber) >= 0.4 then "fair" else "poor" end),
                feature_test_results: $tests
            }
        }')
    
    echo "$functional_results" > "${ARTIFACTS_DIR}/reality_validation/functional_reality.json"
    
    # Log functional summary
    local functionality_status
    functionality_status=$(echo "$functional_results" | jq -r '.functional_reality_test.functionality_status')
    
    log_info "Functional Reality Test: $working_features/$total_features features working (${functional_score}%)"
    log_info "Functionality Status: $functionality_status"
    
    return 0
}

# Individual test functions for deployment reality
test_clean_environment_setup() {
    log_debug "Testing clean environment setup..."
    
    # Check if we can create and access test directories
    local test_dir
    test_dir=$(mktemp -d 2>/dev/null) || return 1
    
    # Test basic file operations
    echo "test" > "${test_dir}/test.txt" 2>/dev/null || return 1
    [[ -f "${test_dir}/test.txt" ]] || return 1
    [[ "$(cat "${test_dir}/test.txt")" == "test" ]] || return 1
    
    # Cleanup
    rm -rf "$test_dir" 2>/dev/null || return 1
    
    return 0
}

test_dependency_resolution() {
    log_debug "Testing dependency resolution..."
    
    # Check for package.json and try to verify dependencies
    if [[ -f "package.json" ]]; then
        # Check if node_modules exists or can be created
        if [[ -d "node_modules" ]] || command -v npm >/dev/null 2>&1; then
            # Try to check dependencies without installing
            npm list --depth=0 >/dev/null 2>&1 && return 0
            # If that fails, check if we can at least parse package.json
            jq -r '.dependencies' package.json >/dev/null 2>&1 && return 0
        fi
    fi
    
    # Check for Python requirements
    if [[ -f "requirements.txt" ]]; then
        command -v python3 >/dev/null 2>&1 && command -v pip >/dev/null 2>&1 && return 0
    fi
    
    # Check for other common dependency files
    if [[ -f "Cargo.toml" ]] && command -v cargo >/dev/null 2>&1; then
        return 0
    fi
    
    # Default to success if no specific dependency requirements found
    return 0
}

test_configuration_validity() {
    log_debug "Testing configuration validity..."
    
    # Check for common configuration files
    local config_valid=false
    
    # Check JSON configuration files
    for config_file in config.json .env.example package.json; do
        if [[ -f "$config_file" ]]; then
            case "$config_file" in
                *.json)
                    if jq empty "$config_file" 2>/dev/null; then
                        config_valid=true
                        break
                    fi
                    ;;
                *)
                    # For non-JSON files, just check if they exist and are readable
                    if [[ -r "$config_file" ]]; then
                        config_valid=true
                        break
                    fi
                    ;;
            esac
        fi
    done
    
    $config_valid && return 0 || return 1
}

test_service_startup() {
    log_debug "Testing service startup..."
    
    # Check if there are startup scripts or commands
    if [[ -f "package.json" ]]; then
        # Check for start script
        if jq -r '.scripts.start' package.json 2>/dev/null | grep -v "null" >/dev/null; then
            return 0
        fi
        
        # Check for dev script
        if jq -r '.scripts.dev' package.json 2>/dev/null | grep -v "null" >/dev/null; then
            return 0
        fi
    fi
    
    # Check for common service files
    for service_file in docker-compose.yml Dockerfile Procfile; do
        if [[ -f "$service_file" ]]; then
            return 0
        fi
    done
    
    # Check for executable files
    if find . -maxdepth 1 -type f -executable | grep -v "^\\./\\." | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

test_basic_functionality() {
    log_debug "Testing basic functionality..."
    
    # Check if there are any source files that suggest functionality
    local has_functionality=false
    
    # Check for source code files
    if find . -name "*.js" -o -name "*.ts" -o -name "*.py" -o -name "*.go" -o -name "*.rs" -o -name "*.java" -o -name "*.c" -o -name "*.cpp" | head -1 >/dev/null 2>&1; then
        has_functionality=true
    fi
    
    # Check for HTML files (web functionality)
    if find . -name "*.html" | head -1 >/dev/null 2>&1; then
        has_functionality=true
    fi
    
    $has_functionality && return 0 || return 1
}

# Individual test functions for user journey scenarios
test_new_user_onboarding() {
    log_debug "Testing new user onboarding..."
    
    # Check for README or getting started documentation
    for readme in README.md README.txt README.rst readme.md getting-started.md; do
        if [[ -f "$readme" ]]; then
            # Check if README has installation or setup instructions
            if grep -i "install\|setup\|getting started\|quickstart" "$readme" >/dev/null 2>&1; then
                return 0
            fi
        fi
    done
    
    return 1
}

test_core_functionality_workflow() {
    log_debug "Testing core functionality workflow..."
    
    # Look for main entry points or executable functionality
    if [[ -f "package.json" ]]; then
        # Check for main entry point
        if jq -r '.main' package.json 2>/dev/null | grep -v "null" >/dev/null; then
            local main_file
            main_file=$(jq -r '.main' package.json 2>/dev/null)
            [[ -f "$main_file" ]] && return 0
        fi
    fi
    
    # Check for common main files
    for main_file in index.js main.py app.py server.js main.go; do
        if [[ -f "$main_file" ]]; then
            return 0
        fi
    done
    
    return 1
}

test_integration_scenarios() {
    log_debug "Testing integration scenarios..."
    
    # Look for integration test files or API definitions
    if find . -name "*integration*" -o -name "*api*" -o -name "*test*" | grep -E "\\.(js|ts|py|json)$" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    # Check for API specification files
    for api_file in openapi.yml swagger.json api.json; do
        if [[ -f "$api_file" ]]; then
            return 0
        fi
    done
    
    return 1
}

test_error_recovery_scenarios() {
    log_debug "Testing error recovery scenarios..."
    
    # Look for error handling code or test files
    if find . -name "*.js" -o -name "*.ts" -o -name "*.py" | xargs grep -l "try.*catch\|except\|error" 2>/dev/null | head -1 >/dev/null; then
        return 0
    fi
    
    return 1
}

# Individual test functions for functional reality
test_authentication_feature() {
    log_debug "Testing authentication feature..."
    
    # Look for authentication-related files
    if find . -name "*auth*" -o -name "*login*" -o -name "*user*" | grep -E "\\.(js|ts|py)$" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

test_data_processing_feature() {
    log_debug "Testing data processing feature..."
    
    # Look for data processing files
    if find . -name "*data*" -o -name "*process*" -o -name "*transform*" | grep -E "\\.(js|ts|py)$" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

test_api_endpoints_feature() {
    log_debug "Testing API endpoints feature..."
    
    # Look for API-related files
    if find . -name "*api*" -o -name "*endpoint*" -o -name "*route*" | grep -E "\\.(js|ts|py)$" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

test_user_interface_feature() {
    log_debug "Testing user interface feature..."
    
    # Look for UI files
    if find . -name "*.html" -o -name "*.css" -o -name "*.jsx" -o -name "*.vue" -o -name "*.svelte" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

test_generic_feature() {
    local feature="$1"
    log_debug "Testing generic feature: $feature"
    
    # Generic test - look for files that might contain the feature
    local feature_lower
    feature_lower=$(echo "$feature" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]//g')
    
    if find . -iname "*$feature_lower*" | head -1 >/dev/null 2>&1; then
        return 0
    fi
    
    return 1
}

# Compile reality validation results
compile_reality_validation_results() {
    log_info "Compiling reality validation results..."
    
    local deployment_results
    local journey_results
    local functional_results
    
    deployment_results=$(cat "${ARTIFACTS_DIR}/reality_validation/deployment_reality.json" 2>/dev/null || echo '{}')
    journey_results=$(cat "${ARTIFACTS_DIR}/reality_validation/user_journey_results.json" 2>/dev/null || echo '{}')
    functional_results=$(cat "${ARTIFACTS_DIR}/reality_validation/functional_reality.json" 2>/dev/null || echo '{}')
    
    # Calculate overall reality score
    local deployment_score
    local journey_score
    local functional_score
    
    deployment_score=$(echo "$deployment_results" | jq -r '.deployment_reality_test.success_rate // 0')
    journey_score=$(echo "$journey_results" | jq -r '.user_journey_validation.success_rate // 0')
    functional_score=$(echo "$functional_results" | jq -r '.functional_reality_test.functional_reality_score // 0')
    
    local overall_score
    overall_score=$(echo "scale=3; ($deployment_score + $journey_score + $functional_score) / 3" | bc -l 2>/dev/null || echo "0")
    
    # Identify critical blockers
    local critical_blockers=()
    
    # Check deployment blockers
    local deployment_readiness
    deployment_readiness=$(echo "$deployment_results" | jq -r '.deployment_reality_test.deployment_readiness // "unknown"')
    if [[ "$deployment_readiness" == "not_ready" ]]; then
        critical_blockers+=("Deployment not ready - users cannot install/run the software")
    fi
    
    # Check journey blockers
    local onboarding_status
    onboarding_status=$(echo "$journey_results" | jq -r '.user_journey_validation.journey_results.new_user_onboarding.status // "unknown"')
    if [[ "$onboarding_status" == "failed" ]]; then
        critical_blockers+=("New user onboarding failed - users cannot get started")
    fi
    
    # Check functionality blockers
    local functional_status
    functional_status=$(echo "$functional_results" | jq -r '.functional_reality_test.functionality_status // "unknown"')
    if [[ "$functional_status" == "poor" ]]; then
        critical_blockers+=("Core functionality broken - software does not work as claimed")
    fi
    
    # Generate final reality assessment
    local reality_assessment
    if (( $(echo "$overall_score >= 0.8" | bc -l) )); then
        reality_assessment="excellent"
    elif (( $(echo "$overall_score >= 0.6" | bc -l) )); then
        reality_assessment="good"
    elif (( $(echo "$overall_score >= 0.4" | bc -l) )); then
        reality_assessment="fair"
    else
        reality_assessment="poor"
    fi
    
    # Compile comprehensive results
    local final_results
    final_results=$(jq -n \
        --argjson deployment "$deployment_results" \
        --argjson journey "$journey_results" \
        --argjson functional "$functional_results" \
        --arg overall_score "$overall_score" \
        --arg assessment "$reality_assessment" \
        --argjson blockers "$(printf '%s\n' "${critical_blockers[@]}" | jq -R . | jq -s .)" \
        '{
            reality_validation_report: {
                session_id: "'$SESSION_ID'",
                validation_timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ"),
                validation_scope: "'$VALIDATION_SCOPE'",
                overall_reality_score: ($overall_score | tonumber),
                reality_assessment: $assessment,
                critical_blockers: $blockers,
                detailed_results: {
                    deployment_reality: $deployment,
                    user_journey_validation: $journey,
                    functional_reality: $functional
                },
                recommendations: (
                    if ($overall_score | tonumber) >= 0.8 then
                        ["Reality validation passed - software works as claimed"]
                    elif ($overall_score | tonumber) >= 0.6 then
                        ["Reality validation mostly successful - minor issues to address"]
                    elif ($overall_score | tonumber) >= 0.4 then
                        ["Reality validation partial - significant issues need fixing"]
                    else
                        ["Reality validation failed - major issues prevent user success"]
                    end
                )
            }
        }')
    
    echo "$final_results" > "${ARTIFACTS_DIR}/reality_validation_final.json"
    
    # Log final summary
    log_info "Reality Validation Summary:"
    log_info "Overall Reality Score: ${overall_score} (${reality_assessment})"
    log_info "Deployment Score: ${deployment_score}"
    log_info "User Journey Score: ${journey_score}"
    log_info "Functional Score: ${functional_score}"
    
    if [[ ${#critical_blockers[@]} -gt 0 ]]; then
        log_warning "Critical Blockers Found:"
        for blocker in "${critical_blockers[@]}"; do
            log_warning "  - $blocker"
        done
    else
        log_success "No critical blockers found"
    fi
    
    return 0
}

# Store results in memory bridge
store_validation_results() {
    log_info "Storing validation results in memory..."
    
    if ! command -v scripts/memory_bridge.sh >/dev/null 2>&1 || [[ ! -f "${SCRIPT_DIR}/memory_bridge.sh" ]]; then
        log_warning "Memory bridge not available - skipping memory storage"
        return 0
    fi
    
    local final_results
    final_results=$(cat "${ARTIFACTS_DIR}/reality_validation_final.json" 2>/dev/null || echo '{}')
    
    if [[ "$final_results" != "{}" ]]; then
        # Store validation results
        scripts/memory_bridge.sh store "intelligence/reality_validation" "validation_$(date +%s)" "$final_results" '{"type": "reality_validation", "session": "'$SESSION_ID'"}' 2>/dev/null || log_warning "Failed to store validation results"
        
        # Store reality patterns for learning
        local reality_patterns
        reality_patterns=$(echo "$final_results" | jq '.reality_validation_report.detailed_results' 2>/dev/null || echo '{}')
        
        if [[ "$reality_patterns" != "{}" ]]; then
            scripts/memory_bridge.sh store "intelligence/patterns" "reality_patterns" "$reality_patterns" '{"type": "reality_patterns", "session": "'$SESSION_ID'"}' 2>/dev/null || log_warning "Failed to store reality patterns"
        fi
        
        # Synchronize memory systems
        scripts/memory_bridge.sh sync 2>/dev/null || log_warning "Memory synchronization failed"
        
        log_success "Validation results stored in memory"
    else
        log_warning "No validation results to store"
    fi
    
    return 0
}

# Main execution function
main() {
    local start_time
    start_time=$(date +%s)
    
    log_info "Starting reality validation with session: $SESSION_ID"
    
    # Initialize environment
    initialize_reality_validation
    
    # Execute validation tests based on scope
    case "$VALIDATION_SCOPE" in
        "comprehensive")
            if [[ "$DEPLOYMENT_TEST" == "true" ]]; then
                test_deployment_reality
            fi
            if [[ "$USER_JOURNEY_TEST" == "true" ]]; then
                test_user_journey_scenarios
            fi
            test_functional_reality
            ;;
        "deployment")
            if [[ "$DEPLOYMENT_TEST" == "true" ]]; then
                test_deployment_reality
            else
                log_warning "Deployment testing disabled"
            fi
            ;;
        "user-journey")
            if [[ "$USER_JOURNEY_TEST" == "true" ]]; then
                test_user_journey_scenarios
            else
                log_warning "User journey testing disabled"
            fi
            ;;
        "functional")
            test_functional_reality
            ;;
        *)
            log_error "Unknown validation scope: $VALIDATION_SCOPE"
            exit 1
            ;;
    esac
    
    # Compile results
    compile_reality_validation_results
    
    # Store results
    store_validation_results
    
    # Calculate execution time
    local end_time
    local duration
    end_time=$(date +%s)
    duration=$((end_time - start_time))
    
    log_success "Reality validation completed in ${duration}s"
    
    # Determine exit status based on results
    local final_results
    final_results=$(cat "${ARTIFACTS_DIR}/reality_validation_final.json" 2>/dev/null || echo '{}')
    
    local overall_score
    overall_score=$(echo "$final_results" | jq -r '.reality_validation_report.overall_reality_score // 0')
    
    local critical_blockers_count
    critical_blockers_count=$(echo "$final_results" | jq -r '.reality_validation_report.critical_blockers // [] | length')
    
    if (( $(echo "$overall_score >= 0.8" | bc -l) )) && [[ "$critical_blockers_count" -eq 0 ]]; then
        log_success "[PARTY] REALITY VALIDATION PASSED: Software works as claimed"
        exit 0
    elif (( $(echo "$overall_score >= 0.6" | bc -l) )) && [[ "$critical_blockers_count" -eq 0 ]]; then
        log_warning "[WARN]  REALITY VALIDATION PARTIAL: Minor issues found"
        exit 1
    elif [[ "$critical_blockers_count" -gt 0 ]]; then
        log_error "[U+1F6AB] REALITY VALIDATION BLOCKED: Critical issues prevent user success"
        exit 2
    else
        log_error "[FAIL] REALITY VALIDATION FAILED: Software does not work as claimed"
        exit 3
    fi
}

# Help function
show_help() {
    cat <<EOF
Reality Validator Script - End-user functionality and deployment testing

USAGE:
    $0 [options]

DESCRIPTION:
    Validates that claimed functionality actually works for real users
    by testing deployment scenarios, user journeys, and functional claims.

OPTIONS:
    -h, --help              Show this help
    -s, --scope <scope>     Validation scope: comprehensive, deployment, user-journey, functional (default: comprehensive)
    -d, --deployment        Enable deployment testing (default: true)
    -u, --user-journey      Enable user journey testing (default: true)
    --session <id>          Custom session ID (default: auto-generated)
    --debug                 Enable debug logging

ENVIRONMENT VARIABLES:
    SESSION_ID              Custom session identifier
    VALIDATION_SCOPE        Validation testing scope
    DEPLOYMENT_TEST         Enable deployment testing (true/false)
    USER_JOURNEY_TEST       Enable user journey testing (true/false)
    DEBUG                   Enable debug output (0/1)

EXIT CODES:
    0 - Reality validation passed, software works as claimed
    1 - Reality validation partial, minor issues found
    2 - Reality validation blocked, critical issues prevent user success
    3 - Reality validation failed, software does not work as claimed

EXAMPLES:
    $0                              # Run comprehensive reality validation
    $0 --scope deployment           # Test only deployment reality
    $0 --scope user-journey --debug # Test user journeys with debug output

INTEGRATION:
    - Tests actual end-user functionality and deployment scenarios
    - Validates claims against real user experience
    - Integrates with SPEK quality framework
    - Stores results in unified memory bridge
EOF
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            exit 0
            ;;
        -s|--scope)
            VALIDATION_SCOPE="$2"
            shift 2
            ;;
        -d|--deployment)
            DEPLOYMENT_TEST="true"
            shift
            ;;
        -u|--user-journey)
            USER_JOURNEY_TEST="true"
            shift
            ;;
        --session)
            SESSION_ID="$2"
            shift 2
            ;;
        --debug)
            DEBUG=1
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            echo "Use --help for usage information" >&2
            exit 1
            ;;
    esac
done

# Validate configuration
case "$VALIDATION_SCOPE" in
    comprehensive|deployment|user-journey|functional) ;;
    *) log_error "Invalid validation scope: $VALIDATION_SCOPE"; exit 1 ;;
esac

# Execute main function
main "$@"