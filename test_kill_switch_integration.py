#!/usr/bin/env python3
"""
Integration Test for Kill Switch System

Tests the complete kill switch system including:
- Import functionality
- Performance validation (<500ms)
- Hardware authentication
- Audit logging
"""

import asyncio
import json
import sys
import time
from pathlib import Path

# Add src to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.safety.kill_switch_system import KillSwitchSystem, TriggerType, KillSwitchEvent
    from src.safety.hardware_auth_manager import HardwareAuthManager, AuthMethod, AuthResult
    print("SUCCESS: Kill switch modules imported successfully")
except ImportError as e:
    print(f"CRITICAL FAILURE: Cannot import kill switch system: {e}")
    print(f"Python path: {sys.path}")
    print(f"Project root: {project_root}")
    print(f"Src exists: {(project_root / 'src').exists()}")
    print(f"Safety exists: {(project_root / 'src' / 'safety').exists()}")
    sys.exit(1)


class MockBroker:
    """Mock broker for testing kill switch functionality."""

    def __init__(self, position_count=5, delay_ms=50):
        self.position_count = position_count
        self.delay_ms = delay_ms
        self.positions = [
            type('Position', (), {
                'symbol': f'STOCK_{i}',
                'qty': 100 + i * 10
            })()
            for i in range(position_count)
        ]
        self.close_calls = []

    async def get_positions(self):
        """Simulate getting positions with configurable delay."""
        await asyncio.sleep(self.delay_ms / 1000.0)
        return self.positions

    async def close_position(self, symbol, qty, side, order_type):
        """Simulate closing position."""
        await asyncio.sleep(10 / 1000.0)  # 10ms per close
        self.close_calls.append({
            'symbol': symbol,
            'qty': qty,
            'side': side,
            'order_type': order_type,
            'timestamp': time.time()
        })
        return True


def test_kill_switch_imports():
    """Test that all imports work correctly."""
    print("\n=== Testing Kill Switch Imports ===")

    # Test enum imports
    assert TriggerType.MANUAL_PANIC
    assert TriggerType.LOSS_LIMIT
    assert AuthMethod.YUBIKEY
    assert AuthMethod.MASTER_KEY
    print(" All enums imported correctly")

    # Test class instantiation
    mock_broker = MockBroker()
    config = {
        'loss_limit': -1000,
        'position_limit': 10000,
        'heartbeat_timeout': 30,
        'audit_file': '.claude/.artifacts/test_kill_switch_audit.jsonl'
    }

    kill_switch = KillSwitchSystem(mock_broker, config)
    assert kill_switch.is_armed()
    print(" KillSwitchSystem instantiation successful")

    auth_config = {
        'allowed_methods': ['master_key'],
        'master_keys': {'default': 'test_key_123'}
    }

    auth_manager = HardwareAuthManager(auth_config)
    available_methods = auth_manager.get_available_methods()
    assert AuthMethod.MASTER_KEY in available_methods
    print(" HardwareAuthManager instantiation successful")

    return True


async def test_kill_switch_performance():
    """Test kill switch performance meets <500ms requirement."""
    print("\n=== Testing Kill Switch Performance ===")

    # Test with different position counts and delays
    test_scenarios = [
        {'positions': 3, 'delay_ms': 20, 'name': 'Light Load'},
        {'positions': 10, 'delay_ms': 30, 'name': 'Medium Load'},
        {'positions': 20, 'delay_ms': 40, 'name': 'Heavy Load'},
    ]

    performance_results = []

    for scenario in test_scenarios:
        print(f"\nTesting {scenario['name']}: {scenario['positions']} positions, {scenario['delay_ms']}ms delay")

        mock_broker = MockBroker(scenario['positions'], scenario['delay_ms'])
        config = {
            'loss_limit': -1000,
            'position_limit': 10000,
            'audit_file': '.claude/.artifacts/performance_test_audit.jsonl'
        }

        kill_switch = KillSwitchSystem(mock_broker, config)

        # Execute kill switch and measure performance
        start_time = time.time()

        result = await kill_switch.trigger_kill_switch(
            TriggerType.MANUAL_PANIC,
            {'test_scenario': scenario['name']}
        )

        actual_time = (time.time() - start_time) * 1000
        reported_time = result.response_time_ms

        performance_results.append({
            'scenario': scenario['name'],
            'positions': scenario['positions'],
            'actual_time_ms': actual_time,
            'reported_time_ms': reported_time,
            'positions_closed': result.positions_flattened,
            'target_met': reported_time < 500,
            'success': result.success
        })

        print(f"  Response Time: {reported_time:.1f}ms (actual: {actual_time:.1f}ms)")
        print(f"  Positions Closed: {result.positions_flattened}/{scenario['positions']}")
        print(f"  Target Met (<500ms): {'' if reported_time < 500 else ''}")
        print(f"  Success: {'' if result.success else ''}")

    # Performance summary
    print(f"\n=== Performance Summary ===")
    all_met_target = all(r['target_met'] for r in performance_results)
    avg_response_time = sum(r['reported_time_ms'] for r in performance_results) / len(performance_results)

    print(f"Average Response Time: {avg_response_time:.1f}ms")
    print(f"All Tests Met <500ms Target: {'' if all_met_target else ''}")

    return performance_results, all_met_target


async def test_hardware_authentication():
    """Test hardware authentication functionality."""
    print("\n=== Testing Hardware Authentication ===")

    # Test configuration
    auth_config = {
        'allowed_methods': ['master_key', 'pin_code', 'yubikey'],
        'master_keys': {
            'default': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',
            'admin': 'test_admin_key_456'
        },
        'pin_code': '1234',
        'max_auth_attempts': 3,
        'lockout_duration': 60
    }

    auth_manager = HardwareAuthManager(auth_config)

    # Test available methods
    available_methods = auth_manager.get_available_methods()
    print(f"Available Methods: {[m.value for m in available_methods]}")

    # Test master key authentication (valid)
    result = await auth_manager.authenticate({
        'method': 'master_key',
        'key': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',
        'user_id': 'test_user'
    })

    print(f"Master Key Auth (Valid): {'' if result.success else ''} - {result.error_message or 'Success'}")

    # Test master key authentication (invalid)
    result = await auth_manager.authenticate({
        'method': 'master_key',
        'key': 'invalid_key',
        'user_id': 'test_user_2'
    })

    print(f"Master Key Auth (Invalid): {'' if not result.success else ''} - {result.error_message or 'Unexpected success'}")

    # Test system status
    status = auth_manager.get_system_status()
    print(f"System Status: {status}")

    # Test YubiKey detection (will show as unavailable unless actual hardware present)
    if 'yubikey' in status['hardware_capabilities']:
        yubikey_available = status['hardware_capabilities']['yubikey']
        print(f"YubiKey Available: {'' if yubikey_available else ''}")

    return True


async def test_integration_scenario():
    """Test complete integration scenario."""
    print("\n=== Testing Complete Integration Scenario ===")

    # Setup
    mock_broker = MockBroker(position_count=5, delay_ms=30)

    kill_switch_config = {
        'loss_limit': -1000,
        'position_limit': 10000,
        'heartbeat_timeout': 30,
        'audit_file': '.claude/.artifacts/integration_test_audit.jsonl'
    }

    auth_config = {
        'allowed_methods': ['master_key'],
        'master_keys': {'emergency': 'emergency_key_789'}
    }

    kill_switch = KillSwitchSystem(mock_broker, kill_switch_config)
    auth_manager = HardwareAuthManager(auth_config)

    # Scenario: Emergency liquidation with authentication
    print("1. Authenticating emergency user...")

    auth_result = await auth_manager.authenticate({
        'method': 'master_key',
        'key': 'emergency_key_789',
        'user_id': 'emergency_operator'
    })

    if not auth_result.success:
        print(f" Authentication failed: {auth_result.error_message}")
        return False

    print(" Authentication successful")

    # Trigger kill switch
    print("2. Triggering kill switch...")

    kill_result = await kill_switch.trigger_kill_switch(
        TriggerType.LOSS_LIMIT,
        {
            'current_loss': -1500,
            'threshold': -1000,
            'authenticated_by': auth_result.user_id
        },
        authentication_method=auth_result.method.value
    )

    print(f" Kill switch executed in {kill_result.response_time_ms:.1f}ms")
    print(f" Positions flattened: {kill_result.positions_flattened}")
    print(f" Success: {kill_result.success}")

    # Verify audit log
    print("3. Verifying audit log...")

    audit_file = Path(kill_switch_config['audit_file'])
    if audit_file.exists():
        with open(audit_file, 'r') as f:
            audit_entries = [json.loads(line) for line in f if line.strip()]

        print(f" Audit entries: {len(audit_entries)}")

        if audit_entries:
            latest_entry = audit_entries[-1]
            print(f" Latest trigger type: {latest_entry['trigger_type']}")
            print(f" Authentication method: {latest_entry['authentication_method']}")

    # Performance metrics
    metrics = kill_switch.get_performance_metrics()
    print(f" Performance metrics: {metrics}")

    return True


async def run_all_tests():
    """Run comprehensive kill switch system tests."""
    print("KILL SWITCH SYSTEM INTEGRATION TEST")
    print("=" * 50)

    test_results = {}

    # Test 1: Imports
    try:
        test_results['imports'] = test_kill_switch_imports()
        print(" Import tests passed")
    except Exception as e:
        print(f" Import tests failed: {e}")
        test_results['imports'] = False
        return test_results

    # Test 2: Performance
    try:
        performance_results, target_met = await test_kill_switch_performance()
        test_results['performance'] = {
            'target_met': target_met,
            'results': performance_results
        }
        print(" Performance tests completed")
    except Exception as e:
        print(f" Performance tests failed: {e}")
        test_results['performance'] = {'target_met': False, 'error': str(e)}

    # Test 3: Hardware Authentication
    try:
        test_results['authentication'] = await test_hardware_authentication()
        print(" Authentication tests passed")
    except Exception as e:
        print(f" Authentication tests failed: {e}")
        test_results['authentication'] = False

    # Test 4: Integration
    try:
        test_results['integration'] = await test_integration_scenario()
        print(" Integration tests passed")
    except Exception as e:
        print(f" Integration tests failed: {e}")
        test_results['integration'] = False

    return test_results


def generate_test_report(test_results):
    """Generate final test report."""
    print("\n" + "=" * 50)
    print("KILL SWITCH SYSTEM TEST REPORT")
    print("=" * 50)

    # Count successful tests
    successful_tests = 0
    total_tests = 0

    for test_name, result in test_results.items():
        total_tests += 1

        if test_name == 'performance':
            success = result.get('target_met', False) if isinstance(result, dict) else False
        else:
            success = bool(result)

        if success:
            successful_tests += 1

        status = "PASS" if success else "FAIL"
        print(f"{test_name.upper()}: {status}")

        if test_name == 'performance' and isinstance(result, dict):
            if 'results' in result:
                for perf_result in result['results']:
                    print(f"  {perf_result['scenario']}: {perf_result['reported_time_ms']:.1f}ms")

    print(f"\nOVERALL: {successful_tests}/{total_tests} tests passed")

    # Reality score calculation
    reality_score = (successful_tests / total_tests) * 10 if total_tests > 0 else 0
    print(f"REALITY SCORE: {reality_score:.1f}/10")

    # Assessment
    if reality_score >= 8.0:
        assessment = "KILL SWITCH SYSTEM FULLY FUNCTIONAL"
    elif reality_score >= 6.0:
        assessment = "KILL SWITCH SYSTEM MOSTLY FUNCTIONAL"
    elif reality_score >= 4.0:
        assessment = "KILL SWITCH SYSTEM PARTIALLY FUNCTIONAL"
    else:
        assessment = "KILL SWITCH SYSTEM CRITICAL FAILURES"

    print(f"ASSESSMENT: {assessment}")

    return {
        'successful_tests': successful_tests,
        'total_tests': total_tests,
        'reality_score': reality_score,
        'assessment': assessment,
        'detailed_results': test_results
    }


if __name__ == '__main__':
    async def main():
        test_results = await run_all_tests()
        final_report = generate_test_report(test_results)

        # Exit with appropriate code
        if final_report['reality_score'] >= 8.0:
            print("\n KILL SWITCH SYSTEM VALIDATION SUCCESSFUL")
            sys.exit(0)
        else:
            print("\n KILL SWITCH SYSTEM VALIDATION FAILED")
            sys.exit(1)

    asyncio.run(main())