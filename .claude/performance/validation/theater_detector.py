from lib.shared.utilities import path_exists
# NASA POT10 Rule 3: Minimize dynamic memory allocation
# Consider using fixed-size arrays or generators for large data processing
#!/usr/bin/env python3
"""
Performance Theater Detection System

Validates genuine performance improvements and prevents fabricated optimization claims.
Provides evidence-based verification of measurable performance gains.
"""

import time
import json
import statistics
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import re

@dataclass
class PerformanceClaim:
    """Performance improvement claim to be validated"""
    claim_id: str
    description: str
    metric_name: str
    baseline_value: float
    optimized_value: float
    improvement_percent: float
    measurement_method: str
    evidence_files: List[str]
    timestamp: float

@dataclass
class ValidationResult:
    """Result of performance claim validation"""
    claim_id: str
    is_valid: bool
    confidence_score: float
    validation_method: str
    evidence_quality: str
    theater_indicators: List[str]
    genuine_indicators: List[str]
    recommendation: str

@dataclass
class TheaterPattern:
    """Pattern that indicates performance theater"""
    pattern_name: str
    description: str
    indicators: List[str]
    severity: str  # 'low', 'medium', 'high'
    detection_method: str

class PerformanceTheaterDetector:
    """
    Advanced theater detection system that validates performance claims
    through evidence analysis, statistical validation, and pattern recognition
    """
    
    def __init__(self):
        self.validation_history: List[ValidationResult] = []
        self.theater_patterns = self._initialize_theater_patterns()
        self.evidence_validators = self._initialize_evidence_validators()
        
        # Statistical thresholds for validation
        self.validation_thresholds = {
            'minimum_improvement': 2.0,      # 2% minimum measurable improvement
            'maximum_believable': 90.0,      # 90% maximum believable improvement
            'confidence_threshold': 0.7,     # 70% confidence required
            'sample_size_minimum': 10,       # Minimum measurements for validity
            'measurement_variance_max': 0.3  # Maximum acceptable variance
        }
        
    def _initialize_theater_patterns(self) -> List[TheaterPattern]:
        """Initialize patterns that indicate performance theater"""
        return [
            TheaterPattern(
                pattern_name="unrealistic_improvements",
                description="Claims improvements that exceed realistic boundaries",
                indicators=[
                    "improvement > 95%",
                    "multiple metrics all improved by >80%",
                    "perfect round numbers (exactly 50%, 75%, etc.)"
                ],
                severity="high",
                detection_method="statistical_analysis"
            ),
            TheaterPattern(
                pattern_name="insufficient_evidence",
                description="Lacks proper measurement evidence",
                indicators=[
                    "no baseline measurements",
                    "single data point comparisons",
                    "missing measurement methodology",
                    "no reproducible test cases"
                ],
                severity="high",
                detection_method="evidence_analysis"
            ),
            TheaterPattern(
                pattern_name="cherry_picked_metrics",
                description="Selective reporting of favorable metrics only",
                indicators=[
                    "only positive metrics reported",
                    "ignoring related performance degradation",
                    "narrow metric selection",
                    "missing context metrics"
                ],
                severity="medium",
                detection_method="context_analysis"
            ),
            TheaterPattern(
                pattern_name="measurement_methodology_flaws",
                description="Flawed or biased measurement approaches",
                indicators=[
                    "inconsistent measurement conditions",
                    "warm-up effects ignored",
                    "system load variations not controlled",
                    "cache effects not considered"
                ],
                severity="medium",
                detection_method="methodology_review"
            ),
            TheaterPattern(
                pattern_name="timing_manipulation",
                description="Manipulated timing or measurement windows",
                indicators=[
                    "suspiciously consistent improvements",
                    "timing measurements with unrealistic precision",
                    "identical improvement ratios across different metrics",
                    "improvements that contradict system constraints"
                ],
                severity="high",
                detection_method="timing_analysis"
            )
        ]
    
    def _initialize_evidence_validators(self) -> Dict[str, Any]:
        """Initialize evidence validation methods"""
        return {
            'benchmark_data': self._validate_benchmark_data,
            'profiling_reports': self._validate_profiling_reports,
            'measurement_logs': self._validate_measurement_logs,
            'statistical_analysis': self._validate_statistical_analysis,
            'reproducibility_tests': self._validate_reproducibility
        }
    
    def validate_performance_claim(self, claim: PerformanceClaim) -> ValidationResult:
        """Comprehensive validation of performance improvement claim"""
        
        # Initialize validation result
        validation_result = ValidationResult(
            claim_id=claim.claim_id,
            is_valid=False,
            confidence_score=0.0,
            validation_method="comprehensive_analysis",
            evidence_quality="unknown",
            theater_indicators=[],
            genuine_indicators=[],
            recommendation=""
        )
        
        # Step 1: Statistical plausibility check
        statistical_score = self._validate_statistical_plausibility(claim)
        
        # Step 2: Evidence quality assessment
        evidence_score = self._assess_evidence_quality(claim)
        
        # Step 3: Theater pattern detection
        theater_indicators = self._detect_theater_patterns(claim)
        
        # Step 4: Genuine improvement indicators
        genuine_indicators = self._detect_genuine_indicators(claim)
        
        # Calculate overall confidence score
        confidence_score = self._calculate_confidence_score(
            statistical_score, evidence_score, theater_indicators, genuine_indicators
        )
        
        # Determine validity
        is_valid = (
            confidence_score >= self.validation_thresholds['confidence_threshold'] and
            len(theater_indicators) == 0 and
            len(genuine_indicators) >= 2
        )
        
        # Generate recommendation
        recommendation = self._generate_recommendation(
            claim, statistical_score, evidence_score, theater_indicators, genuine_indicators
        )
        
        # Update validation result
        validation_result.is_valid = is_valid
        validation_result.confidence_score = confidence_score
        validation_result.evidence_quality = self._categorize_evidence_quality(evidence_score)
        validation_result.theater_indicators = theater_indicators
        validation_result.genuine_indicators = genuine_indicators
        validation_result.recommendation = recommendation
        
        # Store validation history
        self.validation_history.append(validation_result)
        
        return validation_result
    
    def _validate_statistical_plausibility(self, claim: PerformanceClaim) -> float:
        """Validate statistical plausibility of performance claim"""
        plausibility_score = 1.0
        
        # Check improvement magnitude
        improvement = abs(claim.improvement_percent)
        
        if improvement < self.validation_thresholds['minimum_improvement']:
            plausibility_score *= 0.3  # Too small to be meaningful
        elif improvement > self.validation_thresholds['maximum_believable']:
            plausibility_score *= 0.1  # Too large to be believable
        elif improvement > 50.0:
            plausibility_score *= 0.7  # Large improvements need strong evidence
        
        # Check for suspicious round numbers
        if improvement in [25.0, 50.0, 75.0, 90.0, 95.0]:
            plausibility_score *= 0.6  # Suspiciously round
        
        # Check relationship between baseline and optimized values
        if claim.baseline_value <= 0 or claim.optimized_value <= 0:
            plausibility_score *= 0.2  # Invalid baseline/optimized values
        
        # Check for impossible improvements (e.g., negative time)
        if claim.metric_name.lower() in ['time', 'latency', 'duration'] and claim.optimized_value < 0:
            plausibility_score = 0.0  # Impossible
        
        return max(0.0, min(1.0, plausibility_score))
    
    def _assess_evidence_quality(self, claim: PerformanceClaim) -> float:
        """Assess quality of evidence provided for performance claim"""
        evidence_score = 0.0
        
        # Check for evidence files
        if not claim.evidence_files:
            return 0.1  # No evidence provided
        
        evidence_quality_points = 0
        total_possible_points = 0
        
        # Evaluate each evidence file
        for evidence_file in claim.evidence_files:
            total_possible_points += 5  # Maximum points per file
            
            if path_exists(evidence_file):
                evidence_quality_points += 1  # File exists
                
                # Analyze file content if possible
                try:
                    with open(evidence_file, 'r') as f:
                        content = f.read()
                        
                    # Check for measurement data
                    if re.search(r'\d+\.\d+', content):
                        evidence_quality_points += 1  # Contains numerical data
                    
                    # Check for multiple measurements
                    if content.count('\n') > 10:
                        evidence_quality_points += 1  # Substantial data
                    
                    # Check for timestamps
                    if re.search(r'\d{4}-\d{2}-\d{2}|\d+\.\d{10}', content):
                        evidence_quality_points += 1  # Contains timestamps
                    
                    # Check for methodology description
                    if any(keyword in content.lower() for keyword in 
                          ['baseline', 'measurement', 'benchmark', 'test', 'methodology']):
                        evidence_quality_points += 1  # Contains methodology
                        
                except Exception:
                    pass  # File not readable, no additional points
        
        # Calculate evidence score
        if total_possible_points > 0:
            evidence_score = evidence_quality_points / total_possible_points
        
        # Bonus for measurement method description
        if claim.measurement_method and len(claim.measurement_method) > 20:
            evidence_score = min(1.0, evidence_score + 0.2)
        
        return evidence_score
    
    def _detect_theater_patterns(self, claim: PerformanceClaim) -> List[str]:
        """Detect patterns that indicate performance theater"""
        detected_patterns = []
        
        for pattern in self.theater_patterns:
            if self._check_theater_pattern(claim, pattern):
                detected_patterns.append(pattern.pattern_name)
        
        return detected_patterns
    
    def _check_theater_pattern(self, claim: PerformanceClaim, pattern: TheaterPattern) -> bool:
        """Check if a specific theater pattern is present"""
        
        if pattern.pattern_name == "unrealistic_improvements":
            improvement = abs(claim.improvement_percent)
            return (improvement > 95.0 or 
                   improvement in [25.0, 50.0, 75.0, 90.0, 95.0])
        
        elif pattern.pattern_name == "insufficient_evidence":
            return (len(claim.evidence_files) == 0 or 
                   not claim.measurement_method or
                   len(claim.measurement_method) < 10)
        
        elif pattern.pattern_name == "measurement_methodology_flaws":
            method = claim.measurement_method.lower()
            return not any(keyword in method for keyword in 
                          ['baseline', 'multiple', 'average', 'repeated', 'controlled'])
        
        elif pattern.pattern_name == "timing_manipulation":
            # Check for suspiciously precise improvements
            improvement = claim.improvement_percent
            return (improvement == round(improvement, 0) and 
                   improvement % 5 == 0 and improvement > 20)
        
        return False
    
    def _detect_genuine_indicators(self, claim: PerformanceClaim) -> List[str]:
        """Detect indicators of genuine performance improvements"""
        genuine_indicators = []
        
        # Statistical confidence indicators
        if (self.validation_thresholds['minimum_improvement'] <= 
            abs(claim.improvement_percent) <= 
            self.validation_thresholds['maximum_believable']):
            genuine_indicators.append("realistic_improvement_magnitude")
        
        # Evidence quality indicators
        if len(claim.evidence_files) >= 2:
            genuine_indicators.append("multiple_evidence_sources")
        
        # Methodology indicators
        if claim.measurement_method:
            method = claim.measurement_method.lower()
            if any(keyword in method for keyword in 
                  ['baseline', 'controlled', 'repeated', 'statistical']):
                genuine_indicators.append("robust_methodology")
        
        # Precision indicators (not too precise, indicating real measurement)
        improvement = claim.improvement_percent
        if improvement != round(improvement, 0) and improvement % 5 != 0:
            genuine_indicators.append("realistic_measurement_precision")
        
        # Context indicators
        if any(path_exists(f) for f in claim.evidence_files):
            genuine_indicators.append("evidence_files_exist")
        
        return genuine_indicators
    
    def _calculate_confidence_score(self, statistical_score: float, evidence_score: float,
                                  theater_indicators: List[str], 
                                  genuine_indicators: List[str]) -> float:
        """Calculate overall confidence score for performance claim"""
        
        # Base score from statistical and evidence analysis
        base_score = (statistical_score * 0.4 + evidence_score * 0.6)
        
        # Penalty for theater indicators
        theater_penalty = len(theater_indicators) * 0.3
        base_score = max(0.0, base_score - theater_penalty)
        
        # Bonus for genuine indicators
        genuine_bonus = min(0.3, len(genuine_indicators) * 0.1)
        confidence_score = min(1.0, base_score + genuine_bonus)
        
        return confidence_score
    
    def _categorize_evidence_quality(self, evidence_score: float) -> str:
        """Categorize evidence quality based on score"""
        if evidence_score >= 0.8:
            return "excellent"
        elif evidence_score >= 0.6:
            return "good"
        elif evidence_score >= 0.4:
            return "fair"
        elif evidence_score >= 0.2:
            return "poor"
        else:
            return "insufficient"
    
    def _generate_recommendation(self, claim: PerformanceClaim, 
                               statistical_score: float, evidence_score: float,
                               theater_indicators: List[str], 
                               genuine_indicators: List[str]) -> str:
        """Generate actionable recommendation based on validation results"""
        
        if len(theater_indicators) > 0:
            return (f"REJECT: Performance theater detected. "
                   f"Indicators: {', '.join(theater_indicators)}. "
                   f"Provide genuine evidence with proper methodology.")
        
        if evidence_score < 0.3:
            return ("INSUFFICIENT EVIDENCE: Provide comprehensive measurement data, "
                   "baseline comparisons, and detailed methodology.")
        
        if statistical_score < 0.5:
            return ("STATISTICAL CONCERNS: Improvement claims appear implausible. "
                   "Verify measurements and provide additional validation.")
        
        if len(genuine_indicators) < 2:
            return ("NEEDS VALIDATION: Provide additional evidence such as "
                   "multiple measurement runs, reproducibility tests, or "
                   "independent verification.")
        
        if evidence_score >= 0.7 and statistical_score >= 0.7 and len(genuine_indicators) >= 3:
            return "ACCEPT: Performance improvement claim validated with high confidence."
        
        return "CONDITIONAL ACCEPT: Performance improvement appears genuine but requires monitoring."
    
    def validate_multiple_claims(self, claims: List[PerformanceClaim]) -> Dict[str, Any]:
        """Validate multiple performance claims and detect systemic patterns"""
        
        validation_results = []
        for claim in claims:
            result = self.validate_performance_claim(claim)
            validation_results.append(result)
        
        # Analyze patterns across multiple claims
        systemic_analysis = self._analyze_systemic_patterns(claims, validation_results)
        
        # Generate summary
        summary = {
            'total_claims': len(claims),
            'validated_claims': len([r for r in validation_results if r.is_valid]  # TODO: Consider limiting size with itertools.islice()),
            'rejected_claims': len([r for r in validation_results if not r.is_valid]  # TODO: Consider limiting size with itertools.islice()),
            'average_confidence': statistics.mean([r.confidence_score for r in validation_results]  # TODO: Consider limiting size with itertools.islice()),
            'common_theater_patterns': self._identify_common_patterns(validation_results),
            'systemic_analysis': systemic_analysis,
            'individual_results': [asdict(result) for result in validation_results]  # TODO: Consider limiting size with itertools.islice()
        }
        
        return summary
    
    def _analyze_systemic_patterns(self, claims: List[PerformanceClaim], 
                                  results: List[ValidationResult]) -> Dict[str, Any]:
        """Analyze patterns across multiple claims that might indicate systematic theater"""
        
        # Check for suspicious patterns across claims
        improvements = [claim.improvement_percent for claim in claims]  # TODO: Consider limiting size with itertools.islice()
        
        systemic_indicators = []
        
        # All improvements suspiciously similar
        if len(improvements) > 1 and statistics.stdev(improvements) < 2.0:
            systemic_indicators.append("uniform_improvements")
        
        # All improvements are round numbers
        if all(imp == round(imp, 0) for imp in improvements):
            systemic_indicators.append("all_round_numbers")
        
        # Escalating improvement claims
        if len(improvements) > 2 and all(improvements[i] < improvements[i+1] 
                                       for i in range(len(improvements)-1)):
            systemic_indicators.append("escalating_claims")
        
        # Multiple claims with insufficient evidence
        insufficient_evidence_count = len([r for r in results 
                                         if r.evidence_quality in ['poor', 'insufficient']  # TODO: Consider limiting size with itertools.islice()])
        if insufficient_evidence_count > len(results) * 0.7:
            systemic_indicators.append("systematic_insufficient_evidence")
        
        return {
            'systemic_theater_indicators': systemic_indicators,
            'improvement_variance': statistics.stdev(improvements) if len(improvements) > 1 else 0,
            'evidence_quality_distribution': {
                quality: len([r for r in results if r.evidence_quality == quality]  # TODO: Consider limiting size with itertools.islice())
                for quality in ['excellent', 'good', 'fair', 'poor', 'insufficient']
            }
        }
    
    def _identify_common_patterns(self, results: List[ValidationResult]) -> Dict[str, int]:
        """Identify most common theater patterns across all validations"""
        pattern_counts = {}
        
        for result in results:
            for pattern in result.theater_indicators:
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        return dict(sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True))
    
    def export_validation_report(self, claims: List[PerformanceClaim]) -> str:
        """Export comprehensive validation report"""
        
        # Validate all claims
        validation_summary = self.validate_multiple_claims(claims)
        
        # Generate comprehensive report
        report = {
            'report_metadata': {
                'generation_timestamp': time.time(),
                'generation_date': datetime.now().isoformat(),
                'detector_version': "1.0.0",
                'validation_thresholds': self.validation_thresholds
            },
            'executive_summary': {
                'total_claims_analyzed': validation_summary['total_claims'],
                'claims_validated': validation_summary['validated_claims'],
                'claims_rejected': validation_summary['rejected_claims'],
                'overall_confidence': validation_summary['average_confidence'],
                'theater_detection_rate': (validation_summary['rejected_claims'] / 
                                         validation_summary['total_claims']) if validation_summary['total_claims'] > 0 else 0
            },
            'detailed_analysis': validation_summary,
            'theater_patterns_detected': self.theater_patterns,
            'validation_methodology': {
                'statistical_validation': "Plausibility analysis of improvement magnitudes",
                'evidence_assessment': "Quality analysis of supporting documentation",
                'pattern_detection': "Recognition of known theater patterns",
                'confidence_calculation': "Weighted scoring based on multiple factors"
            }
        }
        
        # Export to file
        timestamp = int(time.time())
        report_file = f".claude/performance/validation/theater_detection_report_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        return report_file

def main():
    """Demonstrate performance theater detection system"""
    print("=== Performance Theater Detection System ===")
    
    detector = PerformanceTheaterDetector()
    
    # Create sample performance claims for testing
    claims = [
        PerformanceClaim(
            claim_id="claim_001",
            description="AST traversal optimization",
            metric_name="traversal_time",
            baseline_value=1000.0,
            optimized_value=150.0,
            improvement_percent=85.0,
            measurement_method="Single measurement comparison",
            evidence_files=[],
            timestamp=time.time()
        ),
        PerformanceClaim(
            claim_id="claim_002", 
            description="Memory usage reduction",
            metric_name="memory_usage",
            baseline_value=512.0,
            optimized_value=384.7,
            improvement_percent=24.8,
            measurement_method="Controlled baseline with 50 repeated measurements, statistical analysis",
            evidence_files=["baseline_measurements.json", "optimized_measurements.json"],
            timestamp=time.time()
        ),
        PerformanceClaim(
            claim_id="claim_003",
            description="Cache hit rate improvement",
            metric_name="cache_hit_rate",
            baseline_value=45.0,
            optimized_value=90.0,
            improvement_percent=100.0,
            measurement_method="Before and after comparison",
            evidence_files=["cache_stats.log"],
            timestamp=time.time()
        )
    ]
    
    print(f"Analyzing {len(claims)} performance claims...")
    
    # Validate claims
    validation_summary = detector.validate_multiple_claims(claims)
    
    print(f"\nValidation Results:")
    print(f"  Total claims: {validation_summary['total_claims']}")
    print(f"  Validated: {validation_summary['validated_claims']}")
    print(f"  Rejected: {validation_summary['rejected_claims']}")
    print(f"  Average confidence: {validation_summary['average_confidence']:.2f}")
    
    print(f"\nCommon theater patterns detected:")
    for pattern, count in validation_summary['common_theater_patterns'].items():
        print(f"  {pattern}: {count} occurrences")
    
    # Export detailed report
    report_file = detector.export_validation_report(claims)
    print(f"\nDetailed validation report exported to: {report_file}")
    
    # Show individual claim results
    print(f"\nIndividual Claim Analysis:")
    for result in validation_summary['individual_results']:
        print(f"  Claim {result['claim_id']}: {'VALID' if result['is_valid'] else 'INVALID'} "
              f"(confidence: {result['confidence_score']:.2f})")
        print(f"    Recommendation: {result['recommendation']}")

if __name__ == "__main__":
    main()