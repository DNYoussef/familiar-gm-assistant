# NASA POT10 Rule 3: Minimize dynamic memory allocation
# Consider using fixed-size arrays or generators for large data processing
#!/usr/bin/env python3
"""
DFARS 252.204-7012 Compliance Implementation Framework
Phase 2: Defense Federal Acquisition Regulation Supplement compliance for CUI protection
"""

import ast
import json
from lib.shared.utilities import get_logger
logger = get_logger(__name__)
        self.findings: List[DFARSFinding] = []
        self.cui_assets: List[CUIAsset] = []
        self.compliance_metrics = {
            'controls_implemented': 0,
            'controls_total': len(DFARSControl),
            'findings_critical': 0,
            'findings_high': 0,
            'findings_medium': 0,
            'findings_low': 0,
            'cui_assets_identified': 0,
            'cui_assets_protected': 0,
            'compliance_score': 0.0
        }

    def scan_access_control_implementation(self) -> List[DFARSFinding]:
        """Scan for DFARS 3.1.1 Access Control implementations"""
        findings = []

        # Check for authentication mechanisms
        auth_files = []
        for py_file in self.project_path.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', '.security_backups']):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Look for authentication patterns
                auth_patterns = [
                    'authenticate', 'login', 'session', 'token', 'credential',
                    'password', 'oauth', 'jwt', 'auth', 'verify'
                ]

                if any(pattern in content.lower() for pattern in auth_patterns):
                    auth_files.append(str(py_file))

                # Check for hardcoded credentials (DFARS violation)
                hardcoded_patterns = [
                    r'password\s*=\s*["\'][^"\']+["\']',
                    r'api_key\s*=\s*["\'][^"\']+["\']',
                    r'secret\s*=\s*["\'][^"\']+["\']'
                ]

                for pattern in hardcoded_patterns:
                    import re
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    for match in matches:
                        line_num = content[:match.start()].count('\n') + 1
                        findings.append(DFARSFinding(
                            control_id=DFARSControl.ACCESS_CONTROL.value,
                            control_name="Access Control",
                            finding_type="violation",
                            severity="critical",
                            description="Hardcoded credentials detected",
                            file_path=str(py_file),
                            line_number=line_num,
                            remediation="Move credentials to secure configuration or environment variables",
                            cui_impact=True
                        ))

            except Exception as e:
                self.logger.warning(f"Error scanning {py_file}: {e}")

        # Check for proper access control implementation
        if len(auth_files) == 0:
            findings.append(DFARSFinding(
                control_id=DFARSControl.ACCESS_CONTROL.value,
                control_name="Access Control",
                finding_type="gap",
                severity="high",
                description="No authentication mechanisms detected",
                file_path="system-wide",
                line_number=0,
                remediation="Implement user authentication and access control systems",
                cui_impact=True
            ))

        return findings

    def scan_audit_accountability(self) -> List[DFARSFinding]:
        """Scan for DFARS 3.3.1 Audit and Accountability implementations"""
        findings = []

        # Check for logging implementations
        logging_files = []
        for py_file in self.project_path.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', '.security_backups']):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Look for logging patterns
                if any(pattern in content for pattern in ['logging.', 'logger.', 'log(']):
                    logging_files.append(str(py_file))

                # Check for security event logging
                security_events = ['login', 'logout', 'failed_auth', 'access_denied', 'privilege_escalation']
                security_logging = any(event in content.lower() for event in security_events)

                if 'logging' in content and not security_logging:
                    findings.append(DFARSFinding(
                        control_id=DFARSControl.AUDIT_ACCOUNTABILITY.value,
                        control_name="Audit and Accountability",
                        finding_type="gap",
                        severity="medium",
                        description="Logging present but no security event logging detected",
                        file_path=str(py_file),
                        line_number=0,
                        remediation="Add security event logging for authentication and authorization events",
                        cui_impact=True
                    ))

            except Exception as e:
                continue

        # Check for audit log retention
        audit_retention_files = [f for f in logging_files if 'retention' in Path(f).read_text(encoding='utf-8', errors='ignore').lower()]  # TODO: Consider limiting size with itertools.islice()

        if len(audit_retention_files) == 0:
            findings.append(DFARSFinding(
                control_id=DFARSControl.AUDIT_ACCOUNTABILITY.value,
                control_name="Audit and Accountability",
                finding_type="gap",
                severity="medium",
                description="No audit log retention policy detected",
                file_path="system-wide",
                line_number=0,
                remediation="Implement audit log retention for minimum 1 year as per DFARS requirements",
                cui_impact=True
            ))

        return findings

    def scan_incident_response(self) -> List[DFARSFinding]:
        """Scan for DFARS 3.6.1 Incident Response implementations"""
        findings = []

        # Check for incident response procedures
        incident_files = []
        for py_file in self.project_path.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', '.security_backups']):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Look for incident response patterns
                incident_patterns = [
                    'incident', 'security_event', 'breach', 'alert', 'notification',
                    'escalation', 'response_team', 'forensic'
                ]

                if any(pattern in content.lower() for pattern in incident_patterns):
                    incident_files.append(str(py_file))

            except Exception:
                continue

        # Check for 72-hour reporting capability (DFARS requirement)
        if len(incident_files) == 0:
            findings.append(DFARSFinding(
                control_id=DFARSControl.INCIDENT_RESPONSE.value,
                control_name="Incident Response",
                finding_type="gap",
                severity="critical",
                description="No incident response procedures detected",
                file_path="system-wide",
                line_number=0,
                remediation="Implement incident response procedures with 72-hour reporting capability",
                cui_impact=True
            ))

        return findings

    def scan_system_communications_protection(self) -> List[DFARSFinding]:
        """Scan for DFARS 3.13.1 System and Communications Protection"""
        findings = []

        # Check for encryption implementations
        crypto_files = []
        for py_file in self.project_path.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', '.security_backups']):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Look for encryption patterns
                crypto_patterns = [
                    'encrypt', 'decrypt', 'ssl', 'tls', 'https', 'cryptography',
                    'hashlib', 'secrets', 'fernet', 'aes', 'rsa'
                ]

                if any(pattern in content.lower() for pattern in crypto_patterns):
                    crypto_files.append(str(py_file))

                # Check for weak encryption
                weak_patterns = ['md5', 'sha1', 'des', 'rc4']
                for weak in weak_patterns:
                    if weak in content.lower():
                        line_num = content.lower().find(weak)
                        line_num = content[:line_num].count('\n') + 1
                        findings.append(DFARSFinding(
                            control_id=DFARSControl.SYSTEM_COMMS_PROTECTION.value,
                            control_name="System and Communications Protection",
                            finding_type="violation",
                            severity="high",
                            description=f"Weak cryptographic algorithm detected: {weak}",
                            file_path=str(py_file),
                            line_number=line_num,
                            remediation="Replace with FIPS 140-2 approved algorithms (AES, SHA-256, etc.)",
                            cui_impact=True
                        ))

            except Exception:
                continue

        return findings

    def identify_cui_assets(self) -> List[CUIAsset]:
        """Identify files that may contain Controlled Unclassified Information"""
        cui_assets = []

        # Patterns that might indicate CUI
        cui_indicators = {
            'privacy': ['ssn', 'social_security', 'personal_data', 'pii', 'phi'],
            'proprietary': ['proprietary', 'confidential', 'trade_secret', 'internal'],
            'export_controlled': ['itar', 'ear', 'export_control', 'technical_data'],
            'financial': ['credit_card', 'payment', 'financial_data', 'account_number']
        }

        for py_file in self.project_path.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', '.security_backups']):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read().lower()

                for category, indicators in cui_indicators.items():
                    if any(indicator in content for indicator in indicators):
                        cui_level = CUILevel.BASIC
                        if category == 'privacy':
                            cui_level = CUILevel.PRIVACY
                        elif category == 'proprietary':
                            cui_level = CUILevel.PROPRIETARY

                        cui_assets.append(CUIAsset(
                            file_path=str(py_file),
                            cui_level=cui_level,
                            classification_rationale=f"Contains {category} indicators",
                            access_controls=["role_based_access", "encryption_at_rest"],
                            handling_requirements=["access_logging", "data_loss_prevention"],
                            retention_period=365 * 3  # 3 years default
                        ))
                        break

            except Exception:
                continue

        return cui_assets

    def calculate_compliance_score(self) -> float:
        """Calculate overall DFARS compliance score"""
        total_controls = len(DFARSControl)
        critical_findings = len([f for f in self.findings if f.severity == 'critical']  # TODO: Consider limiting size with itertools.islice())
        high_findings = len([f for f in self.findings if f.severity == 'high']  # TODO: Consider limiting size with itertools.islice())

        # Deduct points for findings
        score = 100.0
        score -= critical_findings * 15  # 15 points per critical
        score -= high_findings * 5      # 5 points per high

        # Minimum score is 0
        score = max(0.0, score)

        return score / 100.0

    def run_full_dfars_scan(self) -> Dict[str, Any]:
        """Run complete DFARS compliance scan"""
        self.logger.info("Starting DFARS 252.204-7012 compliance scan")

        start_time = datetime.now()

        # Run all control scans
        self.findings.extend(self.scan_access_control_implementation())
        self.findings.extend(self.scan_audit_accountability())
        self.findings.extend(self.scan_incident_response())
        self.findings.extend(self.scan_system_communications_protection())

        # Identify CUI assets
        self.cui_assets = self.identify_cui_assets()

        # Calculate metrics
        self.compliance_metrics.update({
            'findings_critical': len([f for f in self.findings if f.severity == 'critical']  # TODO: Consider limiting size with itertools.islice()),
            'findings_high': len([f for f in self.findings if f.severity == 'high']  # TODO: Consider limiting size with itertools.islice()),
            'findings_medium': len([f for f in self.findings if f.severity == 'medium']  # TODO: Consider limiting size with itertools.islice()),
            'findings_low': len([f for f in self.findings if f.severity == 'low']  # TODO: Consider limiting size with itertools.islice()),
            'cui_assets_identified': len(self.cui_assets),
            'compliance_score': self.calculate_compliance_score()
        })

        end_time = datetime.now()
        scan_duration = (end_time - start_time).total_seconds()

        self.logger.info(f"DFARS scan completed in {scan_duration:.2f} seconds")

        return {
            'timestamp': end_time.isoformat(),
            'scan_duration_seconds': scan_duration,
            'compliance_score': self.compliance_metrics['compliance_score'],
            'findings': [
                {
                    'control_id': f.control_id,
                    'control_name': f.control_name,
                    'finding_type': f.finding_type,
                    'severity': f.severity,
                    'description': f.description,
                    'file_path': f.file_path,
                    'line_number': f.line_number,
                    'remediation': f.remediation,
                    'cui_impact': f.cui_impact
                } for f in self.findings
            ]  # TODO: Consider limiting size with itertools.islice(),
            'cui_assets': [
                {
                    'file_path': asset.file_path,
                    'cui_level': asset.cui_level.value,
                    'classification_rationale': asset.classification_rationale,
                    'access_controls': asset.access_controls,
                    'handling_requirements': asset.handling_requirements,
                    'retention_period': asset.retention_period
                } for asset in self.cui_assets
            ]  # TODO: Consider limiting size with itertools.islice(),
            'metrics': self.compliance_metrics,
            'summary': {
                'total_findings': len(self.findings),
                'critical_findings': self.compliance_metrics['findings_critical'],
                'high_findings': self.compliance_metrics['findings_high'],
                'cui_assets_found': len(self.cui_assets),
                'compliance_percentage': self.compliance_metrics['compliance_score'] * 100,
                'dfars_ready': self.compliance_metrics['compliance_score'] >= 0.85
            }
        }

    def generate_dfars_remediation_plan(self) -> Dict[str, Any]:
        """Generate prioritized remediation plan for DFARS compliance"""

        # Group findings by severity and control
        critical_findings = [f for f in self.findings if f.severity == 'critical']  # TODO: Consider limiting size with itertools.islice()
        high_findings = [f for f in self.findings if f.severity == 'high']  # TODO: Consider limiting size with itertools.islice()

        remediation_phases = {
            'phase_1_critical': {
                'timeline': '0-30 days',
                'priority': 'IMMEDIATE',
                'findings': critical_findings,
                'effort_estimate': len(critical_findings) * 8,  # 8 hours per critical
                'business_risk': 'Contract non-compliance, security breaches'
            },
            'phase_2_high': {
                'timeline': '30-60 days',
                'priority': 'HIGH',
                'findings': high_findings,
                'effort_estimate': len(high_findings) * 4,  # 4 hours per high
                'business_risk': 'Audit findings, security gaps'
            },
            'phase_3_optimization': {
                'timeline': '60-90 days',
                'priority': 'MEDIUM',
                'findings': [f for f in self.findings if f.severity in ['medium', 'low']  # TODO: Consider limiting size with itertools.islice()],
                'effort_estimate': len([f for f in self.findings if f.severity in ['medium', 'low']  # TODO: Consider limiting size with itertools.islice()]) * 2,
                'business_risk': 'Process inefficiencies'
            }
        }

        return {
            'remediation_phases': remediation_phases,
            'total_effort_hours': sum(phase['effort_estimate'] for phase in remediation_phases.values()),
            'estimated_completion': '90 days',
            'compliance_target': '85% DFARS compliance score',
            'next_steps': [
                'Begin Phase 1 critical findings remediation',
                'Implement CUI handling procedures',
                'Establish incident response team',
                'Deploy audit logging infrastructure',
                'Schedule DFARS compliance validation'
            ]
        }

def main():
    """Main execution function for DFARS compliance framework"""

    print("DFARS 252.204-7012 COMPLIANCE FRAMEWORK")
    print("=" * 60)
    print("Defense Federal Acquisition Regulation Supplement")
    print("Phase 2: CUI Protection Implementation")
    print()

    framework = DFARSComplianceFramework()

    # Run compliance scan
    results = framework.run_full_dfars_scan()

    # Generate remediation plan
    remediation = framework.generate_dfars_remediation_plan()

    # Display results
    print("DFARS COMPLIANCE SCAN RESULTS")
    print("-" * 40)
    print(f"Compliance Score: {results['summary']['compliance_percentage']:.1f}%")
    print(f"Total Findings: {results['summary']['total_findings']}")
    print(f"Critical: {results['summary']['critical_findings']}")
    print(f"High: {results['summary']['high_findings']}")
    print(f"CUI Assets: {results['summary']['cui_assets_found']}")
    print(f"DFARS Ready: {results['summary']['dfars_ready']}")

    print(f"\nREMEDIATION TIMELINE")
    print("-" * 40)
    print(f"Total Effort: {remediation['total_effort_hours']} hours")
    print(f"Estimated Completion: {remediation['estimated_completion']}")
    print(f"Target Compliance: {remediation['compliance_target']}")

    # Save results
    artifacts_dir = Path('.claude/.artifacts')
    artifacts_dir.mkdir(parents=True, exist_ok=True)

    with open(artifacts_dir / 'dfars_compliance_results.json', 'w') as f:
        json.dump({
            'scan_results': results,
            'remediation_plan': remediation
        }, f, indent=2)

    print(f"\nResults saved to: {artifacts_dir / 'dfars_compliance_results.json'}")

    return results['summary']['dfars_ready']

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)