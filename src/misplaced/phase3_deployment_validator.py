from lib.shared.utilities import path_exists
#!/usr/bin/env python3
"""
Phase 3 Deployment Validator
Final validation of Phase 3 implementation and deployment readiness
Target: 85%+ CI/CD success rate, comprehensive monitoring, automated recovery
"""

import json
import subprocess
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional


class Phase3DeploymentValidator:
    """Validates Phase 3 deployment readiness and actual vs theoretical performance."""
    
    def __init__(self):
        self.phase3_requirements = {
            'e2e_validation_suite': {'required': True, 'weight': 0.15},
            'workflow_health_monitoring': {'required': True, 'weight': 0.20},
            'automated_rollback_system': {'required': True, 'weight': 0.20},
            'performance_regression_detection': {'required': True, 'weight': 0.15},
            'security_compliance_auditor': {'required': True, 'weight': 0.15},
            'enhanced_quality_gates': {'required': True, 'weight': 0.10},
            'parallel_orchestrator_deployed': {'required': True, 'weight': 0.05}
        }
        
        self.success_targets = {
            'ci_cd_success_rate': 0.85,      # 85% target
            'mttr_minutes': 2,               # Mean Time To Recovery < 2 minutes
            'monitoring_coverage': 0.90,     # 90% workflow coverage
            'automation_coverage': 0.80,     # 80% automated recovery
            'performance_consistency': 0.90  # 90% performance consistency
        }
        
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'validation_type': 'phase3_deployment_validation',
            'component_validation': {},
            'performance_validation': {},
            'deployment_readiness': {},
            'actual_vs_theoretical': {},
            'overall_success_rate': 0.0,
            'deployment_ready': False,
            'recommendations': []
        }
    
    def validate_phase3_components(self) -> Dict[str, Any]:
        """Validate all Phase 3 components are implemented and functional."""
        print("Validating Phase 3 components...")
        
        component_validation = {}
        
        # Check E2E validation suite
        e2e_script = Path('scripts/e2e_validation_suite.py')
        component_validation['e2e_validation_suite'] = {
            'implemented': e2e_script.exists(),
            'functional': self._test_script_functionality('scripts/e2e_validation_suite.py'),
            'weight': self.phase3_requirements['e2e_validation_suite']['weight']
        }
        
        # Check workflow health monitoring
        monitoring_workflow = Path('.github/workflows/monitoring-dashboard.yml')
        component_validation['workflow_health_monitoring'] = {
            'implemented': monitoring_workflow.exists(),
            'functional': self._validate_workflow_config(monitoring_workflow),
            'weight': self.phase3_requirements['workflow_health_monitoring']['weight']
        }
        
        # Check automated rollback system
        rollback_workflow = Path('.github/workflows/rollback-automation.yml')
        component_validation['automated_rollback_system'] = {
            'implemented': rollback_workflow.exists(),
            'functional': self._validate_workflow_config(rollback_workflow),
            'weight': self.phase3_requirements['automated_rollback_system']['weight']
        }
        
        # Check performance regression detection
        perf_script = Path('scripts/performance_regression_detector.py')
        component_validation['performance_regression_detection'] = {
            'implemented': perf_script.exists(),
            'functional': self._test_script_functionality('scripts/performance_regression_detector.py'),
            'weight': self.phase3_requirements['performance_regression_detection']['weight']
        }
        
        # Check security compliance auditor
        security_script = Path('scripts/security_compliance_auditor.py')
        component_validation['security_compliance_auditor'] = {
            'implemented': security_script.exists(),
            'functional': self._test_script_functionality('scripts/security_compliance_auditor.py'),
            'weight': self.phase3_requirements['security_compliance_auditor']['weight']
        }
        
        # Check enhanced quality gates
        quality_gates_workflow = Path('.github/workflows/enhanced-quality-gates.yml')
        component_validation['enhanced_quality_gates'] = {
            'implemented': quality_gates_workflow.exists(),
            'functional': self._validate_workflow_config(quality_gates_workflow),
            'weight': self.phase3_requirements['enhanced_quality_gates']['weight']
        }
        
        # Check parallel orchestrator deployment
        parallel_orchestrator = Path('.github/workflows/quality-orchestrator-parallel.yml')
        sequential_orchestrator = Path('.github/workflows/quality-orchestrator.yml')
        
        # Check if parallel is primary (should be the one triggered by push/PR)
        parallel_is_primary = False
        if parallel_orchestrator.exists():
            try:
                with open(parallel_orchestrator, 'r') as f:
                    parallel_content = f.read()
                
                # Check if it has push/pull_request triggers
                parallel_is_primary = 'on:\\n  push:' in parallel_content or 'on:\\n  pull_request:' in parallel_content
                
            except Exception as e:
                print(f'Warning: Could not validate parallel orchestrator: {e}')
        
        component_validation['parallel_orchestrator_deployed'] = {
            'implemented': parallel_orchestrator.exists(),
            'functional': parallel_is_primary,
            'weight': self.phase3_requirements['parallel_orchestrator_deployed']['weight'],
            'details': {
                'parallel_orchestrator_exists': parallel_orchestrator.exists(),
                'sequential_orchestrator_exists': sequential_orchestrator.exists(),
                'parallel_is_primary': parallel_is_primary
            }
        }
        
        self.validation_results['component_validation'] = component_validation
        return component_validation
    
    def _test_script_functionality(self, script_path: str) -> bool:
        """Test if a Python script is functional by doing basic syntax/import check."""
        try:
            # Basic syntax check
            result = subprocess.run([
                sys.executable, '-m', 'py_compile', script_path
            ], capture_output=True, text=True, timeout=10)
            
            return result.returncode == 0
            
        except Exception as e:
            print(f'Warning: Could not test script {script_path}: {e}')
            return False
    
    def _validate_workflow_config(self, workflow_path: Path) -> bool:
        """Validate workflow configuration is syntactically correct."""
        if not workflow_path.exists():
            return False
        
        try:
            import yaml
            with open(workflow_path, 'r', encoding='utf-8') as f:
                yaml.safe_load(f)
            return True
            
        except Exception as e:
            print(f'Warning: Workflow validation failed for {workflow_path}: {e}')
            return False
    
    def validate_actual_vs_theoretical_performance(self) -> Dict[str, Any]:
        """Compare actual performance against theoretical Phase 2/3 targets."""
        print("Validating actual vs theoretical performance...")
        
        # Load performance baselines and current metrics
        phase2_report = Path('.claude/.artifacts/phase2_validation_report.json')
        monitoring_report = Path('.claude/.artifacts/monitoring/workflow_health_dashboard.json')
        performance_report = Path('.claude/.artifacts/monitoring/performance_regression_report.json')
        
        actual_vs_theoretical = {
            'execution_time_comparison': {},
            'success_rate_comparison': {},
            'resource_efficiency_comparison': {},
            'cost_comparison': {},
            'theoretical_targets_met': False,
            'actual_performance_score': 0.0
        }
        
        # Theoretical targets from Phase 2
        theoretical_targets = {
            'execution_time_minutes': 55.0,     # Phase 2 target
            'success_rate': 0.85,               # Phase 3 target
            'memory_efficiency': 0.85,          # Phase 2 achieved
            'cost_reduction_percent': 35.0      # Phase 2 target
        }
        
        # Try to load actual performance data
        actual_metrics = self._collect_actual_performance_metrics(
            monitoring_report, performance_report
        )
        
        # Execution time comparison
        actual_exec_time = actual_metrics.get('execution_time_minutes', 65.0)  # Fallback estimate
        theoretical_exec_time = theoretical_targets['execution_time_minutes']
        
        actual_vs_theoretical['execution_time_comparison'] = {
            'theoretical_target': theoretical_exec_time,
            'actual_measured': actual_exec_time,
            'performance_delta': actual_exec_time - theoretical_exec_time,
            'performance_ratio': theoretical_exec_time / actual_exec_time if actual_exec_time > 0 else 0,
            'target_met': actual_exec_time <= theoretical_exec_time * 1.1  # Within 10% tolerance
        }
        
        # Success rate comparison
        actual_success_rate = actual_metrics.get('success_rate', 0.75)  # Fallback estimate
        theoretical_success_rate = theoretical_targets['success_rate']
        
        actual_vs_theoretical['success_rate_comparison'] = {
            'theoretical_target': theoretical_success_rate,
            'actual_measured': actual_success_rate,
            'performance_delta': actual_success_rate - theoretical_success_rate,
            'target_met': actual_success_rate >= theoretical_success_rate
        }
        
        # Resource efficiency comparison (estimated)
        actual_efficiency = actual_metrics.get('memory_efficiency', 0.80)  # Fallback estimate
        theoretical_efficiency = theoretical_targets['memory_efficiency']
        
        actual_vs_theoretical['resource_efficiency_comparison'] = {
            'theoretical_target': theoretical_efficiency,
            'actual_estimated': actual_efficiency,
            'efficiency_delta': actual_efficiency - theoretical_efficiency,
            'target_met': actual_efficiency >= theoretical_efficiency * 0.95  # Within 5% tolerance
        }
        
        # Cost comparison (estimated based on execution time)
        exec_time_ratio = actual_exec_time / theoretical_exec_time if theoretical_exec_time > 0 else 1.0
        estimated_cost_reduction = theoretical_targets['cost_reduction_percent'] / exec_time_ratio
        
        actual_vs_theoretical['cost_comparison'] = {
            'theoretical_cost_reduction': theoretical_targets['cost_reduction_percent'],
            'estimated_actual_reduction': estimated_cost_reduction,
            'cost_delta': estimated_cost_reduction - theoretical_targets['cost_reduction_percent'],
            'target_met': estimated_cost_reduction >= theoretical_targets['cost_reduction_percent'] * 0.9
        }
        
        # Overall assessment
        targets_met = [
            actual_vs_theoretical['execution_time_comparison']['target_met'],
            actual_vs_theoretical['success_rate_comparison']['target_met'],
            actual_vs_theoretical['resource_efficiency_comparison']['target_met'],
            actual_vs_theoretical['cost_comparison']['target_met']
        ]
        
        actual_vs_theoretical['theoretical_targets_met'] = sum(targets_met) >= 3  # At least 3/4 targets met
        actual_vs_theoretical['targets_met_count'] = sum(targets_met)
        actual_vs_theoretical['actual_performance_score'] = sum(targets_met) / len(targets_met)
        
        self.validation_results['actual_vs_theoretical'] = actual_vs_theoretical
        return actual_vs_theoretical
    
    def _collect_actual_performance_metrics(self, monitoring_report: Path, performance_report: Path) -> Dict[str, Any]:
        """Collect actual performance metrics from monitoring reports."""
        actual_metrics = {}
        
        # Try monitoring report first
        if monitoring_report.exists():
            try:
                with open(monitoring_report, 'r') as f:
                    monitoring_data = json.load(f)
                
                perf_metrics = monitoring_data.get('performance_metrics', {})
                actual_metrics['execution_time_minutes'] = perf_metrics.get('avg_execution_time_minutes', 60.0)
                actual_metrics['success_rate'] = perf_metrics.get('overall_success_rate', 0.75)
                
            except Exception as e:
                print(f'Warning: Could not load monitoring report: {e}')
        
        # Try performance regression report
        if performance_report.exists():
            try:
                with open(performance_report, 'r') as f:
                    perf_data = json.load(f)
                
                current_metrics = perf_data.get('current_metrics', {}).get('system_metrics', {})
                actual_metrics['execution_time_minutes'] = current_metrics.get('avg_execution_time_minutes', 
                                                                              actual_metrics.get('execution_time_minutes', 60.0))
                actual_metrics['success_rate'] = current_metrics.get('overall_success_rate',
                                                                    actual_metrics.get('success_rate', 0.75))
                
            except Exception as e:
                print(f'Warning: Could not load performance regression report: {e}')
        
        # Estimate memory efficiency based on execution time
        exec_time = actual_metrics.get('execution_time_minutes', 60.0)
        baseline_exec_time = 55.0
        estimated_efficiency = 0.85 * (baseline_exec_time / exec_time) if exec_time > 0 else 0.85
        actual_metrics['memory_efficiency'] = min(1.0, max(0.5, estimated_efficiency))
        
        return actual_metrics
    
    def validate_deployment_readiness(self) -> Dict[str, Any]:
        """Validate overall deployment readiness for Phase 3."""
        print("Validating deployment readiness...")
        
        deployment_readiness = {
            'component_readiness_score': 0.0,
            'performance_readiness_score': 0.0,
            'monitoring_readiness_score': 0.0,
            'automation_readiness_score': 0.0,
            'overall_readiness_score': 0.0,
            'deployment_blockers': [],
            'deployment_ready': False
        }
        
        # Component readiness
        component_validation = self.validation_results.get('component_validation', {})
        component_scores = []
        
        for component, validation_data in component_validation.items():
            implemented = validation_data.get('implemented', False)
            functional = validation_data.get('functional', False)
            weight = validation_data.get('weight', 0.1)
            
            component_score = (0.5 if implemented else 0.0) + (0.5 if functional else 0.0)
            component_scores.append(component_score * weight)
            
            if not implemented:
                deployment_readiness['deployment_blockers'].append(f'{component} not implemented')
            elif not functional:
                deployment_readiness['deployment_blockers'].append(f'{component} not functional')
        
        deployment_readiness['component_readiness_score'] = sum(component_scores)
        
        # Performance readiness
        actual_vs_theoretical = self.validation_results.get('actual_vs_theoretical', {})
        performance_score = actual_vs_theoretical.get('actual_performance_score', 0.0)
        deployment_readiness['performance_readiness_score'] = performance_score
        
        if performance_score < 0.75:  # Less than 75% of targets met
            deployment_readiness['deployment_blockers'].append('Performance targets not met')
        
        # Monitoring readiness (check for monitoring artifacts)
        monitoring_artifacts = [
            '.claude/.artifacts/monitoring/workflow_health_dashboard.json',
            '.claude/.artifacts/monitoring/performance_regression_report.json',
            '.claude/.artifacts/monitoring/security_compliance_audit.json'
        ]
        
        monitoring_coverage = sum(1 for artifact in monitoring_artifacts if path_exists(artifact)) / len(monitoring_artifacts)
        deployment_readiness['monitoring_readiness_score'] = monitoring_coverage
        
        if monitoring_coverage < 0.5:
            deployment_readiness['deployment_blockers'].append('Insufficient monitoring coverage')
        
        # Automation readiness (check for automated workflows)
        automation_workflows = [
            '.github/workflows/monitoring-dashboard.yml',
            '.github/workflows/rollback-automation.yml',
            '.github/workflows/enhanced-quality-gates.yml'
        ]
        
        automation_coverage = sum(1 for workflow in automation_workflows if path_exists(workflow)) / len(automation_workflows)
        deployment_readiness['automation_readiness_score'] = automation_coverage
        
        if automation_coverage < 0.75:
            deployment_readiness['deployment_blockers'].append('Insufficient automation coverage')
        
        # Overall readiness score
        readiness_components = [
            deployment_readiness['component_readiness_score'],
            deployment_readiness['performance_readiness_score'],
            deployment_readiness['monitoring_readiness_score'],
            deployment_readiness['automation_readiness_score']
        ]
        
        deployment_readiness['overall_readiness_score'] = sum(readiness_components) / len(readiness_components)
        deployment_readiness['deployment_ready'] = (
            deployment_readiness['overall_readiness_score'] >= 0.8 and 
            len(deployment_readiness['deployment_blockers']) == 0
        )
        
        self.validation_results['deployment_readiness'] = deployment_readiness
        return deployment_readiness
    
    def calculate_overall_success_rate(self) -> float:
        """Calculate overall Phase 3 validation success rate."""
        component_score = self.validation_results['deployment_readiness']['component_readiness_score']
        performance_score = self.validation_results['actual_vs_theoretical']['actual_performance_score']
        
        # Weight components: 60% implementation, 40% performance
        overall_score = (component_score * 0.6) + (performance_score * 0.4)
        
        self.validation_results['overall_success_rate'] = overall_score
        return overall_score
    
    def generate_deployment_recommendations(self) -> List[str]:
        """Generate deployment recommendations based on validation results."""
        recommendations = []
        
        deployment_readiness = self.validation_results['deployment_readiness']
        
        # Address deployment blockers
        blockers = deployment_readiness.get('deployment_blockers', [])
        for blocker in blockers:
            recommendations.append(f'BLOCKER: {blocker} - must be resolved before deployment')
        
        # Component-specific recommendations
        component_validation = self.validation_results['component_validation']
        for component, validation_data in component_validation.items():
            if not validation_data.get('functional', True):
                recommendations.append(f'Fix functionality issues in {component}')
        
        # Performance recommendations
        actual_vs_theoretical = self.validation_results['actual_vs_theoretical']
        
        if not actual_vs_theoretical['execution_time_comparison']['target_met']:
            recommendations.append('Optimize execution time to meet Phase 2 targets')
        
        if not actual_vs_theoretical['success_rate_comparison']['target_met']:
            recommendations.append('Improve CI/CD success rate to reach 85% target')
        
        # Overall readiness recommendations
        overall_score = deployment_readiness['overall_readiness_score']
        
        if overall_score >= 0.9:
            recommendations.append('Phase 3 deployment ready - excellent implementation quality')
        elif overall_score >= 0.8:
            recommendations.append('Phase 3 deployment ready with minor improvements recommended')
        elif overall_score >= 0.7:
            recommendations.append('Phase 3 needs improvement before deployment - address key issues')
        else:
            recommendations.append('Phase 3 not ready for deployment - major issues must be resolved')
        
        # Success recommendations
        if len(blockers) == 0 and overall_score >= 0.8:
            recommendations.append('Proceed with gradual Phase 3 rollout and monitor performance')
            recommendations.append('Implement continuous monitoring and alerting for production')
        
        self.validation_results['recommendations'] = recommendations
        return recommendations
    
    def run_phase3_deployment_validation(self) -> Dict[str, Any]:
        """Run complete Phase 3 deployment validation."""
        print("Starting Phase 3 Deployment Validation")
        print("=" * 50)
        
        # Validate Phase 3 components
        component_validation = self.validate_phase3_components()
        
        # Validate actual vs theoretical performance
        performance_validation = self.validate_actual_vs_theoretical_performance()
        
        # Validate deployment readiness
        deployment_readiness = self.validate_deployment_readiness()
        
        # Calculate overall success rate
        overall_success_rate = self.calculate_overall_success_rate()
        
        # Generate recommendations
        recommendations = self.generate_deployment_recommendations()
        
        return self.validation_results


def main():
    """Main Phase 3 deployment validation execution."""
    print("Phase 3: Deployment Validation")
    print("=" * 50)
    
    validator = Phase3DeploymentValidator()
    results = validator.run_phase3_deployment_validation()
    
    # Save results
    artifacts_dir = Path('.claude/.artifacts/monitoring')
    artifacts_dir.mkdir(exist_ok=True, parents=True)
    
    results_file = artifacts_dir / 'phase3_deployment_validation.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # Print summary
    print("\n" + "=" * 50)
    print("PHASE 3 DEPLOYMENT VALIDATION SUMMARY")
    print("=" * 50)
    
    overall_success = results['overall_success_rate']
    deployment_ready = results['deployment_readiness']['deployment_ready']
    
    print(f"Overall Success Rate: {overall_success:.1%}")
    print(f"Deployment Ready: {'YES' if deployment_ready else 'NO'}")
    
    # Component validation summary
    print("\\nComponent Validation:")
    for component, validation_data in results['component_validation'].items():
        implemented = "[U+2713]" if validation_data['implemented'] else "[U+2717]"
        functional = "[U+2713]" if validation_data['functional'] else "[U+2717]"
        print(f"  - {component}: Implemented {implemented}, Functional {functional}")
    
    # Performance comparison
    print("\\nActual vs Theoretical Performance:")
    perf_comparison = results['actual_vs_theoretical']
    
    exec_comparison = perf_comparison['execution_time_comparison']
    print(f"  - Execution Time: {exec_comparison['actual_measured']:.1f}min vs {exec_comparison['theoretical_target']:.1f}min target")
    
    success_comparison = perf_comparison['success_rate_comparison']
    print(f"  - Success Rate: {success_comparison['actual_measured']:.1%} vs {success_comparison['theoretical_target']:.1%} target")
    
    print(f"  - Targets Met: {perf_comparison['targets_met_count']}/4")
    
    # Deployment blockers
    blockers = results['deployment_readiness']['deployment_blockers']
    if blockers:
        print(f"\\nDeployment Blockers: {len(blockers)}")
        for blocker in blockers[:3]:
            print(f"  - {blocker}")
    
    # Recommendations
    recommendations = results['recommendations']
    if recommendations:
        print(f"\\nTop Recommendations:")
        for i, rec in enumerate(recommendations[:3], 1):
            print(f"  {i}. {rec}")
    
    print(f"\\nDetailed report saved to: {results_file}")
    
    # Exit with status based on deployment readiness
    if deployment_ready:
        print("\\n[ROCKET] PHASE 3 DEPLOYMENT: READY")
        sys.exit(0)
    else:
        print("\\n[WARN]  PHASE 3 DEPLOYMENT: NOT READY")
        sys.exit(1)


if __name__ == '__main__':
    main()