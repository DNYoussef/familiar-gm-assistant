# Gary√óTaleb Data Pipeline

A high-performance, real-time data pipeline system designed for quantitative trading with DPI calculations and antifragility analysis. Built for handling $200 seed capital accounts with institutional-grade data processing capabilities.

## üöÄ Features

### Historical Data Ingestion
- **Multi-Source Support**: Alpaca, Yahoo Finance, Polygon.io
- **5+ Years of Data**: Complete historical market coverage
- **Parallel Processing**: Efficient batch loading with automatic failover
- **Data Validation**: Comprehensive OHLCV integrity checks

### Real-Time Streaming
- **Sub-50ms Latency**: WebSocket-based streaming with intelligent buffering
- **Failover Management**: Automatic source switching and reconnection
- **Stream Buffer**: High-performance circular buffer with backpressure handling
- **Multi-Exchange**: Support for multiple data providers simultaneously

### News Sentiment Pipeline
- **1000+ Articles/Minute**: High-throughput news processing
- **Multi-Source**: NewsAPI, RSS feeds, Alpha Vantage integration
- **Advanced NLP**: FinBERT and domain-specific sentiment analysis
- **Real-Time Processing**: Symbol extraction and relevance scoring

### Options Flow Analysis
- **Unusual Activity Detection**: Statistical analysis for volume anomalies
- **Large Block Identification**: $1M+ threshold monitoring
- **Sweep Detection**: Multi-exchange order pattern recognition
- **IV Spike Alerts**: Implied volatility change monitoring

### Alternative Data Sources
- **Social Sentiment**: Twitter, Reddit, StockTwits integration
- **Economic Indicators**: FRED API for macroeconomic data
- **Cross-Platform Analysis**: Sentiment correlation across platforms
- **Signal Generation**: Automated bullish/bearish signals

### Data Quality & Monitoring
- **Real-Time Validation**: Comprehensive data quality scoring
- **Performance Monitoring**: System resource and component health tracking
- **Quality SLA**: 95%+ data quality compliance monitoring
- **Alert System**: Configurable thresholds with multiple notification channels

## üèóÔ∏è Architecture

```
Gary√óTaleb Data Pipeline
‚îú‚îÄ‚îÄ Sources Layer
‚îÇ   ‚îú‚îÄ‚îÄ Historical Data Loader (Multi-source)
‚îÇ   ‚îú‚îÄ‚îÄ Real-Time Streamer (WebSocket)
‚îÇ   ‚îî‚îÄ‚îÄ Alternative Data Processor
‚îú‚îÄ‚îÄ Processing Layer
‚îÇ   ‚îú‚îÄ‚îÄ News Processor (1000+ articles/min)
‚îÇ   ‚îú‚îÄ‚îÄ Sentiment Processor (FinBERT)
‚îÇ   ‚îî‚îÄ‚îÄ Options Flow Analyzer
‚îú‚îÄ‚îÄ Quality Layer
‚îÇ   ‚îú‚îÄ‚îÄ Data Validator
‚îÇ   ‚îî‚îÄ‚îÄ Quality Monitor
‚îî‚îÄ‚îÄ Monitoring Layer
    ‚îú‚îÄ‚îÄ Pipeline Monitor
    ‚îî‚îÄ‚îÄ Metrics Collector
```

## üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/your-org/gary-taleb-pipeline.git
cd gary-taleb-pipeline

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys
```

### Required Dependencies
```txt
aiohttp>=3.8.0
pandas>=2.0.0
numpy>=1.21.0
websockets>=10.0
nltk>=3.8
transformers>=4.20.0
torch>=1.12.0
psutil>=5.9.0
```

## üîß Configuration

### Environment Variables
```bash
# Data Sources
ALPACA_API_KEY=your_alpaca_key
ALPACA_SECRET_KEY=your_alpaca_secret
POLYGON_API_KEY=your_polygon_key
NEWS_API_KEY=your_newsapi_key
FRED_API_KEY=your_fred_key

# Social Media APIs
TWITTER_BEARER_TOKEN=your_twitter_token
REDDIT_CLIENT_ID=your_reddit_id
REDDIT_CLIENT_SECRET=your_reddit_secret
```

### Pipeline Configuration
```json
{
  "streaming": {
    "buffer_size": 10000,
    "flush_interval": 0.1,
    "max_latency": 0.05
  },
  "processing": {
    "batch_size": 1000,
    "processing_threads": 4,
    "options_flow_threshold": 1000000
  },
  "validation": {
    "quality_threshold": 0.95,
    "completeness_threshold": 0.99
  }
}
```

## üöÄ Quick Start

### Basic Pipeline Usage
```python
import asyncio
from datetime import datetime, timedelta
from src.intelligence.data_pipeline import PipelineOrchestrator

async def main():
    # Initialize pipeline
    pipeline = PipelineOrchestrator()

    # Start all components
    await pipeline.start()

    # Load historical data
    symbols = ["AAPL", "MSFT", "GOOGL", "TSLA"]
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)

    historical_data = await pipeline.load_historical_data(
        symbols, start_date, end_date, "1D"
    )

    # Subscribe to real-time data
    await pipeline.subscribe_real_time(symbols, ["quotes", "trades"])

    # Run pipeline
    await pipeline.run_forever()

if __name__ == "__main__":
    asyncio.run(main())
```

### Historical Data Loading
```python
from src.intelligence.data_pipeline.sources import HistoricalDataLoader

loader = HistoricalDataLoader()
await loader.start()

# Load 5 years of daily data
data = await loader.load_historical_data(
    symbols=["AAPL", "MSFT"],
    start_date=datetime(2019, 1, 1),
    end_date=datetime.now(),
    timeframe="1D",
    source="yahoo"
)

print(f"Loaded {len(data['AAPL'])} days of AAPL data")
```

### Real-Time Streaming
```python
from src.intelligence.data_pipeline.streaming import RealTimeStreamer

streamer = RealTimeStreamer()
await streamer.start()

# Subscribe to real-time quotes
await streamer.subscribe_symbols(["AAPL", "MSFT"], ["quotes"])

# Add data callback
def process_quote(stream_data):
    print(f"Quote for {stream_data.symbol}: {stream_data.data}")

streamer.add_subscriber(process_quote)
```

### News Sentiment Analysis
```python
from src.intelligence.data_pipeline.processing import NewsProcessor, SentimentProcessor

# Start processors
news_processor = NewsProcessor()
sentiment_processor = SentimentProcessor()

await news_processor.start()
await sentiment_processor.start()

# Get recent sentiment for symbols
sentiment_data = {}
for symbol in ["AAPL", "MSFT"]:
    sentiment = sentiment_processor.get_symbol_sentiment(symbol, timeframe="4h")
    sentiment_data[symbol] = sentiment

print("Sentiment Analysis Results:")
for symbol, data in sentiment_data.items():
    print(f"{symbol}: {data['weighted_sentiment']:.2f}")
```

### Options Flow Analysis
```python
from src.intelligence.data_pipeline.processing import OptionsFlowAnalyzer

analyzer = OptionsFlowAnalyzer()
await analyzer.start()

# Get recent unusual activity
alerts = analyzer.get_recent_alerts(hours=24)
print(f"Found {len(alerts)} unusual options activities")

# Get top flow symbols
top_flow = analyzer.get_top_flow_symbols(limit=10)
for item in top_flow:
    print(f"{item['symbol']}: ${item['total_premium']:,.0f} flow")
```

## üìä Monitoring & Metrics

### Pipeline Status
```python
# Get overall pipeline status
status = pipeline.get_pipeline_status()
print(f"Status: {status.status}")
print(f"Components: {status.components_running}/{status.components_total}")
print(f"Uptime: {status.uptime_seconds:.0f}s")

# Get performance metrics
metrics = pipeline.get_performance_metrics()
print(f"System CPU: {metrics['system_metrics']['current_cpu_percent']:.1f}%")
print(f"System Memory: {metrics['system_metrics']['current_memory_percent']:.1f}%")
```

### Data Quality Monitoring
```python
from src.intelligence.data_pipeline.validation import QualityMonitor

monitor = QualityMonitor()
await monitor.start()

# Get quality summary
summary = monitor.get_quality_summary(hours=24)
print(f"Average Quality Score: {summary['average_quality_score']:.2f}")
print(f"SLA Compliance: {summary['sla_compliance']:.1%}")
```

### Prometheus Metrics Export
```python
from src.intelligence.data_pipeline.monitoring import MetricsCollector

collector = MetricsCollector()
await collector.start()

# Export metrics in Prometheus format
prometheus_metrics = collector.export_prometheus_format()
print(prometheus_metrics)
```

## üîç Advanced Usage

### Custom Data Validation Rules
```python
from src.intelligence.data_pipeline.validation import DataValidator

validator = DataValidator()

# Validate OHLCV data
validation_result = validator.validate_ohlcv_data(dataframe, "AAPL")

if not validation_result.passed:
    print(f"Validation failed with score: {validation_result.score:.2f}")
    for issue in validation_result.issues:
        print(f"- {issue['description']}")

# Clean data based on validation results
cleaned_data = validator.clean_data(dataframe, validation_result)
```

### Alternative Data Integration
```python
from src.intelligence.data_pipeline.processing import AlternativeDataProcessor

alt_processor = AlternativeDataProcessor()
await alt_processor.start()

# Get social sentiment
sentiment = alt_processor.get_social_sentiment("AAPL", hours=24)
print(f"Social sentiment: {sentiment['weighted_sentiment']:.2f}")

# Get active alternative signals
signals = alt_processor.get_active_signals(signal_type="social")
for signal in signals[:5]:
    print(f"{signal.symbol}: {signal.direction} ({signal.strength:.2f})")
```

## üìà Performance Characteristics

### Throughput Benchmarks
- **Historical Data**: 10,000+ symbols/hour
- **Real-Time Streaming**: <50ms latency, 1000+ quotes/second
- **News Processing**: 1000+ articles/minute
- **Options Analysis**: Real-time unusual activity detection
- **Data Validation**: 95%+ quality score maintenance

### Resource Requirements
- **CPU**: 4+ cores recommended
- **Memory**: 8GB+ RAM for full pipeline
- **Storage**: 100GB+ for historical data cache
- **Network**: Stable internet for real-time feeds

### Scalability
- **Horizontal**: Multiple pipeline instances
- **Vertical**: Component-level scaling
- **Data Sources**: Unlimited source integration
- **Processing**: Thread-pool based parallel processing

## üõ†Ô∏è Troubleshooting

### Common Issues

#### Connection Failures
```bash
# Check API key configuration
python -c "from src.intelligence.data_pipeline.config import config; print(config.validate_config())"

# Test individual source connectivity
python -c "from src.intelligence.data_pipeline.sources import YahooSource; import asyncio; asyncio.run(YahooSource(config.data_sources['yahoo']).get_real_time_quote('AAPL'))"
```

#### Performance Issues
```python
# Check system resources
metrics = pipeline.get_performance_metrics()
print(f"CPU: {metrics['system_metrics']['current_cpu_percent']:.1f}%")
print(f"Memory: {metrics['system_metrics']['current_memory_percent']:.1f}%")

# Check component health
health = await pipeline.health_check()
for component, status in health['components'].items():
    if not status['healthy']:
        print(f"Unhealthy component: {component}")
```

#### Data Quality Issues
```python
# Check quality metrics
summary = quality_monitor.get_quality_summary(hours=24)
if summary['average_quality_score'] < 0.9:
    print("Quality degradation detected")

# Get recent quality alerts
alerts = quality_monitor.get_recent_alerts(hours=1, severity="critical")
for alert in alerts:
    print(f"Critical alert: {alert['description']}")
```

## üîí Security Considerations

- **API Keys**: Store in environment variables, never commit to code
- **Rate Limiting**: Respect API rate limits to avoid service disruption
- **Data Privacy**: Social media data collection follows platform ToS
- **Network Security**: Use HTTPS/WSS for all external connections
- **Access Control**: Implement proper authentication for monitoring endpoints

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìû Support

For support and questions:
- Create an issue on GitHub
- Check the troubleshooting section
- Review the API documentation

---

Built for the Gary√óTaleb trading system - where DPI meets antifragility for superior risk-adjusted returns.